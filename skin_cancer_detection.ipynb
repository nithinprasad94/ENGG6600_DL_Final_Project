{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15fd5d6c-f38a-4f0d-be11-bf63fbefb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b950702-2346-4d52-96f2-d572f4e7c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Exploration with the 28x28 image csv [FUNCTIONAL]\n",
    "\n",
    "#print(\"Hello\")\n",
    "#project_df = pd.read_csv('data/hmnist_28_28_L.csv')\n",
    "#project_df.head(n=4)\n",
    "#print(project_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf10dcc-dda9-4a15-9e06-d87c9dc584ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train  1135\n",
      "Length of X_val:  378\n",
      "Length of X_test:  379\n",
      "1892\n"
     ]
    }
   ],
   "source": [
    "### Do Train-Validation-Test Split on the Data (eg. 60/20/20)\n",
    "\n",
    "#read csv\n",
    "df = pd.read_csv('./data/skin_cancer_dataset.csv')\n",
    "\n",
    "#separate the features and labels\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "#split 60-40 for training dataset and a temp dataset that will be split to 50-50\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split of the temporary set: 50% for validation and 50% for test set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#sanity check\n",
    "sizes = (len(X_train), len(X_val), len(X_test))\n",
    "print(\"Length of X_train \",sizes[0])\n",
    "print(\"Length of X_val: \",sizes[1])\n",
    "print(\"Length of X_test: \",sizes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ffed339d-a3f0-43b9-bbf8-f9a8d94b2299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "(1135, 1)\n",
      "(378, 1)\n",
      "(379, 1)\n",
      "---------------------\n",
      "(1135, 2)\n",
      "(378, 2)\n",
      "(379, 2)\n",
      "---------------------\n",
      "(1135, 5)\n",
      "(378, 5)\n",
      "(379, 5)\n",
      "---------------------\n",
      "(1135, 10)\n",
      "(378, 10)\n",
      "(379, 10)\n",
      "---------------------\n",
      "(1135, 20)\n",
      "(378, 20)\n",
      "(379, 20)\n",
      "---------------------\n",
      "(1135, 50)\n",
      "(378, 50)\n",
      "(379, 50)\n",
      "---------------------\n",
      "(1135, 100)\n",
      "(378, 100)\n",
      "(379, 100)\n",
      "---------------------\n",
      "(1135, 200)\n",
      "(378, 200)\n",
      "(379, 200)\n",
      "---------------------\n",
      "(1135, 300)\n",
      "(378, 300)\n",
      "(379, 300)\n",
      "---------------------\n",
      "(1135, 400)\n",
      "(378, 400)\n",
      "(379, 400)\n",
      "---------------------\n",
      "(1135, 500)\n",
      "(378, 500)\n",
      "(379, 500)\n",
      "---------------------\n",
      "(1135, 600)\n",
      "(378, 600)\n",
      "(379, 600)\n",
      "---------------------\n",
      "(1135, 700)\n",
      "(378, 700)\n",
      "(379, 700)\n",
      "---------------------\n",
      "(1135, 784)\n",
      "(378, 784)\n",
      "(379, 784)\n",
      "Done PCA\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Apply PCA (fit the PCA matrix using ONLY the training portion of the dataset). Transform each separately\n",
    "# fit pca on the training set only\n",
    "num_pixels = 28*28\n",
    "\n",
    "X_train_pca_vec = []\n",
    "X_test_pca_vec = []\n",
    "X_val_pca_vec = []\n",
    "num_components = [1,2,5,10,20,50,100,200,300,400,500,600,700,784]\n",
    "\n",
    "for i in range(len(num_components)):\n",
    "    #pca = PCA(n_components=min(sizes[0],num_pixels), random_state=42)\n",
    "    pca = PCA(n_components=num_components[i],random_state=42)\n",
    "    pca.fit(X_train)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    X_train_pca_vec.append(X_train_pca)\n",
    "    X_test_pca_vec.append(X_test_pca)\n",
    "    X_val_pca_vec.append(X_val_pca)\n",
    "\n",
    "    #sanity check\n",
    "    print(\"---------------------\")\n",
    "    print(X_train_pca.shape)\n",
    "    print(X_val_pca.shape)\n",
    "    print(X_test_pca.shape)\n",
    "print(\"Done PCA\")\n",
    "\n",
    "\n",
    "#Explained Variance Ratio: Code for this was completed using this reference https://saturncloud.io/blog/what-is-sklearn-pca-explained-variance-and-explained-variance-ratio-difference/#code-example-1\n",
    "expl_var_ratio = pca.explained_variance_ratio_\n",
    "print(type(expl_var_ratio))\n",
    "expl_var_ratio_cumul = np.cumsum(expl_var_ratio)\n",
    "plt.plot(expl_var_ratio[0:100], marker='x')\n",
    "plt.plot(expl_var_ratio_cumul[0:100], marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio by Principal Components')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67f8817c-0e63-429e-bacf-0f035c85b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the PCA-transformed features and labels into PyTorch tensors\n",
    "train_features = torch.tensor(X_train_pca, dtype=torch.float)\n",
    "val_features = torch.tensor(X_val_pca, dtype=torch.float)\n",
    "test_features = torch.tensor(X_test_pca, dtype=torch.float)\n",
    "\n",
    "train_labels = torch.tensor(y_train.values, dtype=torch.long)  # Assuming y_train is a pandas Series\n",
    "val_labels = torch.tensor(y_val.values, dtype=torch.long)  # Assuming y_val is a pandas Series\n",
    "test_labels = torch.tensor(y_test.values, dtype=torch.long)  # Assuming y_test is a pandas Series\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset = TensorDataset(val_features, val_labels)\n",
    "test_dataset = TensorDataset(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0ac55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropout_rate, num_classes=7):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define the network layers as before\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            # layers.append(nn.LayerNorm(size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = size\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da286ef8-1e88-4050-b262-249719f8fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_model(train_dataset, val_dataset, device, params):\n",
    "    # Model initialization\n",
    "    dr = params['dropout_rate']\n",
    "    model = MLP(input_size=params['input_size'], hidden_sizes=params['hidden_sizes'], dropout_rate=dr,num_classes=7).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(params['epochs']):\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            output = model(data)\n",
    "            preds = torch.max(output, 1)[1]\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(val_targets, val_preds)\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    return val_accuracy, model\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():  # Inference mode, no gradients needed\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb6fced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters to be adjusted by user\n",
    "'''\n",
    "#First we fix the hyperparameter grid\n",
    "hyp_baseline = {\n",
    "    'lr': 0.001,\n",
    "    'num_epochs': 50,\n",
    "    'batch_size': 128,\n",
    "    'dropout_rate': 0.5,\n",
    "}\n",
    "\n",
    "#Vary the model architecture\n",
    "model_architecture = {\n",
    "    'hidden_layer_dims': [\n",
    "        [32],\n",
    "        [64],\n",
    "        [128],\n",
    "        [256],\n",
    "        [32,32],\n",
    "        [32,64],\n",
    "        [32,128],\n",
    "        [32,256],\n",
    "        [64,32],\n",
    "        [64,64],\n",
    "        [64,128],\n",
    "        [64,256],\n",
    "        [128,32],\n",
    "        [128,64],\n",
    "        [128,128],\n",
    "        [128,256],\n",
    "        [256,32],\n",
    "        [256,64],\n",
    "        [256,128],\n",
    "        [256,256],\n",
    "        [128,64,128],\n",
    "        [128,64,64,128]\n",
    "    ],\n",
    "}\n",
    "'''\n",
    "\n",
    "#Now we fix vary the hyperparameters\n",
    "hyperparameter_grid = {\n",
    "    'lr': [0.01,0.001,0.0001],\n",
    "    'num_epochs': [20,50,100],\n",
    "    'batch_size': [32,64,128],\n",
    "    'dropout_rate': [0.2,0.5,0.7]\n",
    "}\n",
    "\n",
    "#Keep the model architecture fixed to what we found as a 'good' architecture\n",
    "model_architecture = {\n",
    "    'hidden_layer_dims': [256,128]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16df0fe9-ae7e-4368-af0d-09f94eec8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_accuracy = 0.0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "'''\n",
    "#Trying to find a good model architecture\n",
    "for dims in model_architecture['hidden_layer_dims']:\n",
    "    print(f\"Training with hidden_sizes={dims}\")\n",
    "    params = {\n",
    "        'input_size': X_train_pca.shape[1],\n",
    "        'hidden_sizes': dims,\n",
    "        'lr': hyp_baseline['lr'],\n",
    "        'epochs': hyp_baseline['num_epochs'],  # Keeping epochs fixed for this example\n",
    "        'batch_size': hyp_baseline['batch_size'],\n",
    "        'dropout_rate': hyp_baseline['dropout_rate']\n",
    "    }\n",
    "            \n",
    "    # Train and validate the model with the current set of parameters\n",
    "    val_accuracy, model = train_validate_model(train_dataset, val_dataset, device, params)\n",
    "    \n",
    "    print(f\"Validation accuracy: {val_accuracy}\")\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "'''\n",
    "\n",
    "'''\n",
    "#Trying to find the optimum hyperparameters\n",
    "for lr in hyperparameter_grid['lr']:\n",
    "    for num_epochs in hyperparameter_grid['num_epochs']:\n",
    "        for batch_size in hyperparameter_grid['batch_size']:\n",
    "            for dropout_rate in hyperparameter_grid['dropout_rate']:\n",
    "                print(f\"Training with lr={lr}, hidden_sizes={hidden_sizes}, batch_size={batch_size}\")\n",
    "                params = {\n",
    "                    'input_size': X_train_pca.shape[1],\n",
    "                    'hidden_sizes': model_architecture['hidden_layer_dims'],\n",
    "                    'lr': lr,\n",
    "                    'epochs': num_epochs,  # Keeping epochs fixed for this example\n",
    "                    'batch_size': batch_size,\n",
    "                    'dropout_rate': dropout_rate\n",
    "                }\n",
    "                \n",
    "                # Train and validate the model with the current set of parameters\n",
    "                val_accuracy, model = train_validate_model(train_dataset, val_dataset, device, params)\n",
    "                \n",
    "                print(f\"Validation accuracy: {val_accuracy}\")\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    best_params = params\n",
    "                    best_model = model\n",
    "\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best validation accuracy: {best_accuracy}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb4fbf14-ac29-4666-8705-8401dda18c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.818937301635742\n",
      "Epoch 2, Loss: 4.428957462310791\n",
      "Epoch 3, Loss: 3.9835753440856934\n",
      "Epoch 4, Loss: 3.352837324142456\n",
      "Epoch 5, Loss: 4.211023807525635\n",
      "Epoch 6, Loss: 3.950310468673706\n",
      "Epoch 7, Loss: 3.073199987411499\n",
      "Epoch 8, Loss: 3.3548529148101807\n",
      "Epoch 9, Loss: 3.471167802810669\n",
      "Epoch 10, Loss: 2.792452812194824\n",
      "Epoch 11, Loss: 2.6913650035858154\n",
      "Epoch 12, Loss: 2.1462454795837402\n",
      "Epoch 13, Loss: 2.7645344734191895\n",
      "Epoch 14, Loss: 2.3381707668304443\n",
      "Epoch 15, Loss: 2.1141297817230225\n",
      "Epoch 16, Loss: 2.2603302001953125\n",
      "Epoch 17, Loss: 2.3190128803253174\n",
      "Epoch 18, Loss: 2.654886245727539\n",
      "Epoch 19, Loss: 1.9707167148590088\n",
      "Epoch 20, Loss: 2.1373448371887207\n",
      "Epoch 21, Loss: 1.9456483125686646\n",
      "Epoch 22, Loss: 1.793926477432251\n",
      "Epoch 23, Loss: 1.9995700120925903\n",
      "Epoch 24, Loss: 2.0563857555389404\n",
      "Epoch 25, Loss: 1.9056662321090698\n",
      "Epoch 26, Loss: 1.5636687278747559\n",
      "Epoch 27, Loss: 1.8114997148513794\n",
      "Epoch 28, Loss: 1.5158779621124268\n",
      "Epoch 29, Loss: 1.796427845954895\n",
      "Epoch 30, Loss: 1.6141834259033203\n",
      "Epoch 31, Loss: 2.057605743408203\n",
      "Epoch 32, Loss: 1.6502183675765991\n",
      "Epoch 33, Loss: 1.7562994956970215\n",
      "Epoch 34, Loss: 1.8157671689987183\n",
      "Epoch 35, Loss: 1.644234538078308\n",
      "Epoch 36, Loss: 1.7177387475967407\n",
      "Epoch 37, Loss: 1.719448208808899\n",
      "Epoch 38, Loss: 1.6260457038879395\n",
      "Epoch 39, Loss: 1.6968375444412231\n",
      "Epoch 40, Loss: 1.7476904392242432\n",
      "Epoch 41, Loss: 1.5998121500015259\n",
      "Epoch 42, Loss: 1.8545366525650024\n",
      "Epoch 43, Loss: 1.6980558633804321\n",
      "Epoch 44, Loss: 1.8082317113876343\n",
      "Epoch 45, Loss: 1.7884352207183838\n",
      "Epoch 46, Loss: 1.5281113386154175\n",
      "Epoch 47, Loss: 1.4017785787582397\n",
      "Epoch 48, Loss: 1.4792717695236206\n",
      "Epoch 49, Loss: 1.7465968132019043\n",
      "Epoch 50, Loss: 1.5396562814712524\n",
      "Epoch 51, Loss: 1.4776637554168701\n",
      "Epoch 52, Loss: 1.2928400039672852\n",
      "Epoch 53, Loss: 1.3202948570251465\n",
      "Epoch 54, Loss: 1.592403769493103\n",
      "Epoch 55, Loss: 1.6488635540008545\n",
      "Epoch 56, Loss: 1.6211377382278442\n",
      "Epoch 57, Loss: 1.4695745706558228\n",
      "Epoch 58, Loss: 1.4914286136627197\n",
      "Epoch 59, Loss: 1.3561835289001465\n",
      "Epoch 60, Loss: 1.2400482892990112\n",
      "Epoch 61, Loss: 1.339223861694336\n",
      "Epoch 62, Loss: 1.4578588008880615\n",
      "Epoch 63, Loss: 1.363550066947937\n",
      "Epoch 64, Loss: 1.4284855127334595\n",
      "Epoch 65, Loss: 1.3267525434494019\n",
      "Epoch 66, Loss: 1.3400241136550903\n",
      "Epoch 67, Loss: 1.1652830839157104\n",
      "Epoch 68, Loss: 1.2613539695739746\n",
      "Epoch 69, Loss: 1.3540477752685547\n",
      "Epoch 70, Loss: 1.350358247756958\n",
      "Epoch 71, Loss: 1.3026772737503052\n",
      "Epoch 72, Loss: 1.2746522426605225\n",
      "Epoch 73, Loss: 1.5385253429412842\n",
      "Epoch 74, Loss: 1.5437837839126587\n",
      "Epoch 75, Loss: 1.2227097749710083\n",
      "Epoch 76, Loss: 1.222464680671692\n",
      "Epoch 77, Loss: 1.2727079391479492\n",
      "Epoch 78, Loss: 1.2591861486434937\n",
      "Epoch 79, Loss: 1.5366102457046509\n",
      "Epoch 80, Loss: 1.1826308965682983\n",
      "Epoch 81, Loss: 1.1807044744491577\n",
      "Epoch 82, Loss: 1.2351558208465576\n",
      "Epoch 83, Loss: 1.4401311874389648\n",
      "Epoch 84, Loss: 1.1695122718811035\n",
      "Epoch 85, Loss: 1.1397230625152588\n",
      "Epoch 86, Loss: 1.0143218040466309\n",
      "Epoch 87, Loss: 0.8899427652359009\n",
      "Epoch 88, Loss: 1.0786775350570679\n",
      "Epoch 89, Loss: 1.0977786779403687\n",
      "Epoch 90, Loss: 1.2474616765975952\n",
      "Epoch 91, Loss: 1.26485276222229\n",
      "Epoch 92, Loss: 1.0677437782287598\n",
      "Epoch 93, Loss: 1.476101040840149\n",
      "Epoch 94, Loss: 1.1350904703140259\n",
      "Epoch 95, Loss: 1.0391312837600708\n",
      "Epoch 96, Loss: 1.201082468032837\n",
      "Epoch 97, Loss: 1.1491466760635376\n",
      "Epoch 98, Loss: 1.0056759119033813\n",
      "Epoch 99, Loss: 0.8981779217720032\n",
      "Epoch 100, Loss: 0.8471273183822632\n",
      "Validation Accuracy: 0.3466\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9767d325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 83.35106658935547\n",
      "Epoch 2, Loss: 86.58769989013672\n",
      "Epoch 3, Loss: 80.41014862060547\n",
      "Epoch 4, Loss: 60.69529342651367\n",
      "Epoch 5, Loss: 57.12295150756836\n",
      "Epoch 6, Loss: 59.45261764526367\n",
      "Epoch 7, Loss: 45.194190979003906\n",
      "Epoch 8, Loss: 33.11060333251953\n",
      "Epoch 9, Loss: 39.429927825927734\n",
      "Epoch 10, Loss: 47.08540344238281\n",
      "Epoch 11, Loss: 42.1719970703125\n",
      "Epoch 12, Loss: 39.36521911621094\n",
      "Epoch 13, Loss: 38.1843376159668\n",
      "Epoch 14, Loss: 38.01435089111328\n",
      "Epoch 15, Loss: 24.062284469604492\n",
      "Epoch 16, Loss: 21.597097396850586\n",
      "Epoch 17, Loss: 22.737333297729492\n",
      "Epoch 18, Loss: 22.21658706665039\n",
      "Epoch 19, Loss: 19.515499114990234\n",
      "Epoch 20, Loss: 16.62285041809082\n",
      "Epoch 21, Loss: 17.461807250976562\n",
      "Epoch 22, Loss: 11.721200942993164\n",
      "Epoch 23, Loss: 13.656307220458984\n",
      "Epoch 24, Loss: 13.972796440124512\n",
      "Epoch 25, Loss: 10.51287841796875\n",
      "Epoch 26, Loss: 9.183537483215332\n",
      "Epoch 27, Loss: 8.54679012298584\n",
      "Epoch 28, Loss: 8.418468475341797\n",
      "Epoch 29, Loss: 10.049925804138184\n",
      "Epoch 30, Loss: 6.479970932006836\n",
      "Epoch 31, Loss: 7.076401710510254\n",
      "Epoch 32, Loss: 6.433761119842529\n",
      "Epoch 33, Loss: 7.244376182556152\n",
      "Epoch 34, Loss: 10.773635864257812\n",
      "Epoch 35, Loss: 4.416141033172607\n",
      "Epoch 36, Loss: 3.6770191192626953\n",
      "Epoch 37, Loss: 6.30385684967041\n",
      "Epoch 38, Loss: 6.0829291343688965\n",
      "Epoch 39, Loss: 5.021018981933594\n",
      "Epoch 40, Loss: 7.055850982666016\n",
      "Epoch 41, Loss: 3.9367568492889404\n",
      "Epoch 42, Loss: 4.572750568389893\n",
      "Epoch 43, Loss: 5.03511381149292\n",
      "Epoch 44, Loss: 4.1591477394104\n",
      "Epoch 45, Loss: 2.99951434135437\n",
      "Epoch 46, Loss: 5.000450611114502\n",
      "Epoch 47, Loss: 3.6116116046905518\n",
      "Epoch 48, Loss: 2.6428000926971436\n",
      "Epoch 49, Loss: 2.672285318374634\n",
      "Epoch 50, Loss: 3.174522876739502\n",
      "Epoch 51, Loss: 3.063800573348999\n",
      "Epoch 52, Loss: 2.408853530883789\n",
      "Epoch 53, Loss: 3.973296880722046\n",
      "Epoch 54, Loss: 2.558245897293091\n",
      "Epoch 55, Loss: 2.634921073913574\n",
      "Epoch 56, Loss: 3.0745837688446045\n",
      "Epoch 57, Loss: 3.032212257385254\n",
      "Epoch 58, Loss: 2.569671392440796\n",
      "Epoch 59, Loss: 3.378012180328369\n",
      "Epoch 60, Loss: 3.498384952545166\n",
      "Epoch 61, Loss: 2.186331272125244\n",
      "Epoch 62, Loss: 2.089855194091797\n",
      "Epoch 63, Loss: 2.8961076736450195\n",
      "Epoch 64, Loss: 2.3230764865875244\n",
      "Epoch 65, Loss: 2.205937385559082\n",
      "Epoch 66, Loss: 2.257415294647217\n",
      "Epoch 67, Loss: 3.2208025455474854\n",
      "Epoch 68, Loss: 2.1428263187408447\n",
      "Epoch 69, Loss: 3.015742778778076\n",
      "Epoch 70, Loss: 2.8442840576171875\n",
      "Epoch 71, Loss: 2.332721710205078\n",
      "Epoch 72, Loss: 2.688037157058716\n",
      "Epoch 73, Loss: 2.023670196533203\n",
      "Epoch 74, Loss: 2.3971054553985596\n",
      "Epoch 75, Loss: 2.4962563514709473\n",
      "Epoch 76, Loss: 2.7652828693389893\n",
      "Epoch 77, Loss: 3.6164157390594482\n",
      "Epoch 78, Loss: 1.9120659828186035\n",
      "Epoch 79, Loss: 2.638902425765991\n",
      "Epoch 80, Loss: 2.1554367542266846\n",
      "Epoch 81, Loss: 2.010796070098877\n",
      "Epoch 82, Loss: 2.0711021423339844\n",
      "Epoch 83, Loss: 2.126009941101074\n",
      "Epoch 84, Loss: 2.418382406234741\n",
      "Epoch 85, Loss: 2.09260630607605\n",
      "Epoch 86, Loss: 2.1915030479431152\n",
      "Epoch 87, Loss: 2.6979663372039795\n",
      "Epoch 88, Loss: 2.2705559730529785\n",
      "Epoch 89, Loss: 2.1287548542022705\n",
      "Epoch 90, Loss: 2.703629970550537\n",
      "Epoch 91, Loss: 1.9297116994857788\n",
      "Epoch 92, Loss: 2.022130250930786\n",
      "Epoch 93, Loss: 2.891935348510742\n",
      "Epoch 94, Loss: 1.9623171091079712\n",
      "Epoch 95, Loss: 2.259934663772583\n",
      "Epoch 96, Loss: 2.069597005844116\n",
      "Epoch 97, Loss: 2.076777458190918\n",
      "Epoch 98, Loss: 2.078073024749756\n",
      "Epoch 99, Loss: 2.06784987449646\n",
      "Epoch 100, Loss: 2.0810015201568604\n",
      "Validation Accuracy: 0.1587\n",
      "Epoch 1, Loss: 81.06507110595703\n",
      "Epoch 2, Loss: 67.45989990234375\n",
      "Epoch 3, Loss: 71.9309310913086\n",
      "Epoch 4, Loss: 62.39876937866211\n",
      "Epoch 5, Loss: 46.63167953491211\n",
      "Epoch 6, Loss: 57.010929107666016\n",
      "Epoch 7, Loss: 62.71112823486328\n",
      "Epoch 8, Loss: 36.24692916870117\n",
      "Epoch 9, Loss: 27.229450225830078\n",
      "Epoch 10, Loss: 37.538089752197266\n",
      "Epoch 11, Loss: 46.629920959472656\n",
      "Epoch 12, Loss: 36.49753189086914\n",
      "Epoch 13, Loss: 38.01853561401367\n",
      "Epoch 14, Loss: 42.72343444824219\n",
      "Epoch 15, Loss: 27.206777572631836\n",
      "Epoch 16, Loss: 31.695005416870117\n",
      "Epoch 17, Loss: 27.370698928833008\n",
      "Epoch 18, Loss: 29.68737030029297\n",
      "Epoch 19, Loss: 24.0233097076416\n",
      "Epoch 20, Loss: 20.94814682006836\n",
      "Epoch 21, Loss: 20.756805419921875\n",
      "Epoch 22, Loss: 26.560739517211914\n",
      "Epoch 23, Loss: 13.0909423828125\n",
      "Epoch 24, Loss: 15.507513999938965\n",
      "Epoch 25, Loss: 11.99079418182373\n",
      "Epoch 26, Loss: 12.003446578979492\n",
      "Epoch 27, Loss: 12.413677215576172\n",
      "Epoch 28, Loss: 11.809189796447754\n",
      "Epoch 29, Loss: 11.157096862792969\n",
      "Epoch 30, Loss: 10.254884719848633\n",
      "Epoch 31, Loss: 10.380703926086426\n",
      "Epoch 32, Loss: 8.375679016113281\n",
      "Epoch 33, Loss: 6.439056873321533\n",
      "Epoch 34, Loss: 9.35835075378418\n",
      "Epoch 35, Loss: 7.88314962387085\n",
      "Epoch 36, Loss: 9.474010467529297\n",
      "Epoch 37, Loss: 5.916670799255371\n",
      "Epoch 38, Loss: 7.62001371383667\n",
      "Epoch 39, Loss: 5.8161301612854\n",
      "Epoch 40, Loss: 5.774593830108643\n",
      "Epoch 41, Loss: 4.831972122192383\n",
      "Epoch 42, Loss: 3.315751314163208\n",
      "Epoch 43, Loss: 4.617763996124268\n",
      "Epoch 44, Loss: 4.779219627380371\n",
      "Epoch 45, Loss: 5.584268569946289\n",
      "Epoch 46, Loss: 3.6683008670806885\n",
      "Epoch 47, Loss: 3.00248122215271\n",
      "Epoch 48, Loss: 3.4688265323638916\n",
      "Epoch 49, Loss: 2.273432493209839\n",
      "Epoch 50, Loss: 4.504056930541992\n",
      "Epoch 51, Loss: 3.222768545150757\n",
      "Epoch 52, Loss: 3.5037941932678223\n",
      "Epoch 53, Loss: 3.9208295345306396\n",
      "Epoch 54, Loss: 3.512070894241333\n",
      "Epoch 55, Loss: 3.694218635559082\n",
      "Epoch 56, Loss: 2.4553840160369873\n",
      "Epoch 57, Loss: 3.0612194538116455\n",
      "Epoch 58, Loss: 3.829766035079956\n",
      "Epoch 59, Loss: 3.203273057937622\n",
      "Epoch 60, Loss: 2.651718854904175\n",
      "Epoch 61, Loss: 3.0785305500030518\n",
      "Epoch 62, Loss: 2.5633039474487305\n",
      "Epoch 63, Loss: 2.3422136306762695\n",
      "Epoch 64, Loss: 3.0428242683410645\n",
      "Epoch 65, Loss: 3.5702881813049316\n",
      "Epoch 66, Loss: 2.613729953765869\n",
      "Epoch 67, Loss: 2.479430913925171\n",
      "Epoch 68, Loss: 2.200371026992798\n",
      "Epoch 69, Loss: 2.746809720993042\n",
      "Epoch 70, Loss: 2.070361614227295\n",
      "Epoch 71, Loss: 1.9200488328933716\n",
      "Epoch 72, Loss: 2.627636432647705\n",
      "Epoch 73, Loss: 2.2119433879852295\n",
      "Epoch 74, Loss: 2.2326724529266357\n",
      "Epoch 75, Loss: 2.3911120891571045\n",
      "Epoch 76, Loss: 2.469559907913208\n",
      "Epoch 77, Loss: 3.2861597537994385\n",
      "Epoch 78, Loss: 2.3237814903259277\n",
      "Epoch 79, Loss: 2.274580240249634\n",
      "Epoch 80, Loss: 2.2163236141204834\n",
      "Epoch 81, Loss: 2.2386252880096436\n",
      "Epoch 82, Loss: 2.0175423622131348\n",
      "Epoch 83, Loss: 2.6302125453948975\n",
      "Epoch 84, Loss: 1.8962912559509277\n",
      "Epoch 85, Loss: 1.7700351476669312\n",
      "Epoch 86, Loss: 2.395585775375366\n",
      "Epoch 87, Loss: 2.098262071609497\n",
      "Epoch 88, Loss: 1.9593523740768433\n",
      "Epoch 89, Loss: 2.273449420928955\n",
      "Epoch 90, Loss: 2.557997703552246\n",
      "Epoch 91, Loss: 2.3724822998046875\n",
      "Epoch 92, Loss: 1.9137178659439087\n",
      "Epoch 93, Loss: 2.695580244064331\n",
      "Epoch 94, Loss: 2.0750157833099365\n",
      "Epoch 95, Loss: 2.6938529014587402\n",
      "Epoch 96, Loss: 2.245699167251587\n",
      "Epoch 97, Loss: 2.1135125160217285\n",
      "Epoch 98, Loss: 2.548957347869873\n",
      "Epoch 99, Loss: 2.087050676345825\n",
      "Epoch 100, Loss: 2.3176825046539307\n",
      "Validation Accuracy: 0.1640\n",
      "Epoch 1, Loss: 48.77700424194336\n",
      "Epoch 2, Loss: 51.57781982421875\n",
      "Epoch 3, Loss: 44.2841682434082\n",
      "Epoch 4, Loss: 34.58238220214844\n",
      "Epoch 5, Loss: 41.52662658691406\n",
      "Epoch 6, Loss: 27.89227867126465\n",
      "Epoch 7, Loss: 37.03635025024414\n",
      "Epoch 8, Loss: 39.07228469848633\n",
      "Epoch 9, Loss: 30.168140411376953\n",
      "Epoch 10, Loss: 33.639163970947266\n",
      "Epoch 11, Loss: 23.469175338745117\n",
      "Epoch 12, Loss: 29.251707077026367\n",
      "Epoch 13, Loss: 21.283336639404297\n",
      "Epoch 14, Loss: 24.21194839477539\n",
      "Epoch 15, Loss: 15.716721534729004\n",
      "Epoch 16, Loss: 21.57137107849121\n",
      "Epoch 17, Loss: 17.327495574951172\n",
      "Epoch 18, Loss: 18.253562927246094\n",
      "Epoch 19, Loss: 16.814167022705078\n",
      "Epoch 20, Loss: 19.479839324951172\n",
      "Epoch 21, Loss: 10.404324531555176\n",
      "Epoch 22, Loss: 10.841519355773926\n",
      "Epoch 23, Loss: 15.4144287109375\n",
      "Epoch 24, Loss: 13.207401275634766\n",
      "Epoch 25, Loss: 12.13614559173584\n",
      "Epoch 26, Loss: 8.180283546447754\n",
      "Epoch 27, Loss: 7.700578212738037\n",
      "Epoch 28, Loss: 13.184536933898926\n",
      "Epoch 29, Loss: 8.431339263916016\n",
      "Epoch 30, Loss: 8.987128257751465\n",
      "Epoch 31, Loss: 7.175116539001465\n",
      "Epoch 32, Loss: 7.951260089874268\n",
      "Epoch 33, Loss: 7.659158229827881\n",
      "Epoch 34, Loss: 7.035459995269775\n",
      "Epoch 35, Loss: 5.01973819732666\n",
      "Epoch 36, Loss: 4.260076522827148\n",
      "Epoch 37, Loss: 4.519936561584473\n",
      "Epoch 38, Loss: 4.979241371154785\n",
      "Epoch 39, Loss: 5.157613754272461\n",
      "Epoch 40, Loss: 5.408302307128906\n",
      "Epoch 41, Loss: 5.41982889175415\n",
      "Epoch 42, Loss: 4.990463733673096\n",
      "Epoch 43, Loss: 4.64830207824707\n",
      "Epoch 44, Loss: 4.55758810043335\n",
      "Epoch 45, Loss: 3.958322763442993\n",
      "Epoch 46, Loss: 2.421638250350952\n",
      "Epoch 47, Loss: 3.2323036193847656\n",
      "Epoch 48, Loss: 4.006649494171143\n",
      "Epoch 49, Loss: 3.3924872875213623\n",
      "Epoch 50, Loss: 3.699298620223999\n",
      "Epoch 51, Loss: 4.999881267547607\n",
      "Epoch 52, Loss: 2.476511240005493\n",
      "Epoch 53, Loss: 2.4544427394866943\n",
      "Epoch 54, Loss: 3.1132864952087402\n",
      "Epoch 55, Loss: 2.746800184249878\n",
      "Epoch 56, Loss: 3.090571880340576\n",
      "Epoch 57, Loss: 2.960007667541504\n",
      "Epoch 58, Loss: 3.390871047973633\n",
      "Epoch 59, Loss: 2.8005433082580566\n",
      "Epoch 60, Loss: 2.5472002029418945\n",
      "Epoch 61, Loss: 2.3246798515319824\n",
      "Epoch 62, Loss: 2.2074577808380127\n",
      "Epoch 63, Loss: 2.817936897277832\n",
      "Epoch 64, Loss: 2.458714485168457\n",
      "Epoch 65, Loss: 2.6261208057403564\n",
      "Epoch 66, Loss: 2.5025510787963867\n",
      "Epoch 67, Loss: 2.3665406703948975\n",
      "Epoch 68, Loss: 3.7511165142059326\n",
      "Epoch 69, Loss: 2.122082471847534\n",
      "Epoch 70, Loss: 3.1651365756988525\n",
      "Epoch 71, Loss: 2.382812738418579\n",
      "Epoch 72, Loss: 1.9333754777908325\n",
      "Epoch 73, Loss: 2.0414767265319824\n",
      "Epoch 74, Loss: 1.9532296657562256\n",
      "Epoch 75, Loss: 2.0584657192230225\n",
      "Epoch 76, Loss: 2.034715414047241\n",
      "Epoch 77, Loss: 2.037700891494751\n",
      "Epoch 78, Loss: 2.885129451751709\n",
      "Epoch 79, Loss: 1.8375715017318726\n",
      "Epoch 80, Loss: 2.403430938720703\n",
      "Epoch 81, Loss: 2.2298057079315186\n",
      "Epoch 82, Loss: 2.430481195449829\n",
      "Epoch 83, Loss: 2.322310209274292\n",
      "Epoch 84, Loss: 2.413867473602295\n",
      "Epoch 85, Loss: 2.855656623840332\n",
      "Epoch 86, Loss: 2.402881622314453\n",
      "Epoch 87, Loss: 2.0000901222229004\n",
      "Epoch 88, Loss: 2.4616568088531494\n",
      "Epoch 89, Loss: 2.3757970333099365\n",
      "Epoch 90, Loss: 1.969804048538208\n",
      "Epoch 91, Loss: 2.222144365310669\n",
      "Epoch 92, Loss: 1.784909725189209\n",
      "Epoch 93, Loss: 2.2263503074645996\n",
      "Epoch 94, Loss: 2.34024977684021\n",
      "Epoch 95, Loss: 2.051121950149536\n",
      "Epoch 96, Loss: 2.1218483448028564\n",
      "Epoch 97, Loss: 1.9494552612304688\n",
      "Epoch 98, Loss: 1.9536079168319702\n",
      "Epoch 99, Loss: 1.9016551971435547\n",
      "Epoch 100, Loss: 1.8480637073516846\n",
      "Validation Accuracy: 0.2275\n",
      "Epoch 1, Loss: 43.55732345581055\n",
      "Epoch 2, Loss: 36.68061828613281\n",
      "Epoch 3, Loss: 30.275619506835938\n",
      "Epoch 4, Loss: 33.39096450805664\n",
      "Epoch 5, Loss: 34.249149322509766\n",
      "Epoch 6, Loss: 29.136737823486328\n",
      "Epoch 7, Loss: 19.79400062561035\n",
      "Epoch 8, Loss: 24.78282356262207\n",
      "Epoch 9, Loss: 20.84303855895996\n",
      "Epoch 10, Loss: 23.904197692871094\n",
      "Epoch 11, Loss: 21.175373077392578\n",
      "Epoch 12, Loss: 23.76242446899414\n",
      "Epoch 13, Loss: 11.53499698638916\n",
      "Epoch 14, Loss: 20.619232177734375\n",
      "Epoch 15, Loss: 12.474480628967285\n",
      "Epoch 16, Loss: 12.947697639465332\n",
      "Epoch 17, Loss: 18.66524887084961\n",
      "Epoch 18, Loss: 23.958913803100586\n",
      "Epoch 19, Loss: 12.171857833862305\n",
      "Epoch 20, Loss: 13.755014419555664\n",
      "Epoch 21, Loss: 10.61609172821045\n",
      "Epoch 22, Loss: 9.079258918762207\n",
      "Epoch 23, Loss: 11.597705841064453\n",
      "Epoch 24, Loss: 9.37032413482666\n",
      "Epoch 25, Loss: 9.555638313293457\n",
      "Epoch 26, Loss: 9.321404457092285\n",
      "Epoch 27, Loss: 7.467592716217041\n",
      "Epoch 28, Loss: 6.629306316375732\n",
      "Epoch 29, Loss: 6.534423351287842\n",
      "Epoch 30, Loss: 8.252009391784668\n",
      "Epoch 31, Loss: 4.92606258392334\n",
      "Epoch 32, Loss: 7.322114944458008\n",
      "Epoch 33, Loss: 3.9310812950134277\n",
      "Epoch 34, Loss: 5.419301986694336\n",
      "Epoch 35, Loss: 6.020364761352539\n",
      "Epoch 36, Loss: 4.509284973144531\n",
      "Epoch 37, Loss: 4.515451431274414\n",
      "Epoch 38, Loss: 4.765585899353027\n",
      "Epoch 39, Loss: 4.49789571762085\n",
      "Epoch 40, Loss: 3.6362974643707275\n",
      "Epoch 41, Loss: 4.981955051422119\n",
      "Epoch 42, Loss: 3.348339319229126\n",
      "Epoch 43, Loss: 4.051483631134033\n",
      "Epoch 44, Loss: 2.9097251892089844\n",
      "Epoch 45, Loss: 5.092182159423828\n",
      "Epoch 46, Loss: 3.6345889568328857\n",
      "Epoch 47, Loss: 4.7315545082092285\n",
      "Epoch 48, Loss: 3.239696502685547\n",
      "Epoch 49, Loss: 2.7081587314605713\n",
      "Epoch 50, Loss: 2.5817601680755615\n",
      "Epoch 51, Loss: 2.9629650115966797\n",
      "Epoch 52, Loss: 3.041544198989868\n",
      "Epoch 53, Loss: 2.513427734375\n",
      "Epoch 54, Loss: 2.692533016204834\n",
      "Epoch 55, Loss: 3.0325865745544434\n",
      "Epoch 56, Loss: 2.222130537033081\n",
      "Epoch 57, Loss: 3.041274070739746\n",
      "Epoch 58, Loss: 2.218244791030884\n",
      "Epoch 59, Loss: 2.628931999206543\n",
      "Epoch 60, Loss: 1.5767769813537598\n",
      "Epoch 61, Loss: 2.917616128921509\n",
      "Epoch 62, Loss: 2.5654184818267822\n",
      "Epoch 63, Loss: 1.955283761024475\n",
      "Epoch 64, Loss: 2.98785400390625\n",
      "Epoch 65, Loss: 2.2442126274108887\n",
      "Epoch 66, Loss: 2.198747158050537\n",
      "Epoch 67, Loss: 2.546579122543335\n",
      "Epoch 68, Loss: 2.2983005046844482\n",
      "Epoch 69, Loss: 2.4708433151245117\n",
      "Epoch 70, Loss: 2.0169005393981934\n",
      "Epoch 71, Loss: 2.6363022327423096\n",
      "Epoch 72, Loss: 2.4846718311309814\n",
      "Epoch 73, Loss: 2.363129138946533\n",
      "Epoch 74, Loss: 2.113290548324585\n",
      "Epoch 75, Loss: 1.9173364639282227\n",
      "Epoch 76, Loss: 2.3672232627868652\n",
      "Epoch 77, Loss: 2.516420841217041\n",
      "Epoch 78, Loss: 2.1717841625213623\n",
      "Epoch 79, Loss: 2.021531343460083\n",
      "Epoch 80, Loss: 1.8484443426132202\n",
      "Epoch 81, Loss: 2.322122097015381\n",
      "Epoch 82, Loss: 1.9928666353225708\n",
      "Epoch 83, Loss: 2.2806503772735596\n",
      "Epoch 84, Loss: 2.510660409927368\n",
      "Epoch 85, Loss: 2.2405002117156982\n",
      "Epoch 86, Loss: 2.1022183895111084\n",
      "Epoch 87, Loss: 2.1491336822509766\n",
      "Epoch 88, Loss: 1.8958061933517456\n",
      "Epoch 89, Loss: 2.1143345832824707\n",
      "Epoch 90, Loss: 2.355818510055542\n",
      "Epoch 91, Loss: 2.3097991943359375\n",
      "Epoch 92, Loss: 1.8435314893722534\n",
      "Epoch 93, Loss: 1.98870849609375\n",
      "Epoch 94, Loss: 2.3138420581817627\n",
      "Epoch 95, Loss: 2.18159556388855\n",
      "Epoch 96, Loss: 1.8708075284957886\n",
      "Epoch 97, Loss: 2.5346128940582275\n",
      "Epoch 98, Loss: 2.3710219860076904\n",
      "Epoch 99, Loss: 1.8420557975769043\n",
      "Epoch 100, Loss: 1.8943058252334595\n",
      "Validation Accuracy: 0.1905\n",
      "Epoch 1, Loss: 28.30476188659668\n",
      "Epoch 2, Loss: 27.205730438232422\n",
      "Epoch 3, Loss: 21.154348373413086\n",
      "Epoch 4, Loss: 23.61137580871582\n",
      "Epoch 5, Loss: 24.38646125793457\n",
      "Epoch 6, Loss: 20.48267364501953\n",
      "Epoch 7, Loss: 24.642820358276367\n",
      "Epoch 8, Loss: 19.295635223388672\n",
      "Epoch 9, Loss: 19.212369918823242\n",
      "Epoch 10, Loss: 16.510499954223633\n",
      "Epoch 11, Loss: 15.751026153564453\n",
      "Epoch 12, Loss: 11.821188926696777\n",
      "Epoch 13, Loss: 14.10727596282959\n",
      "Epoch 14, Loss: 14.074711799621582\n",
      "Epoch 15, Loss: 10.953574180603027\n",
      "Epoch 16, Loss: 9.94739055633545\n",
      "Epoch 17, Loss: 9.332133293151855\n",
      "Epoch 18, Loss: 8.771162033081055\n",
      "Epoch 19, Loss: 10.916181564331055\n",
      "Epoch 20, Loss: 10.24751091003418\n",
      "Epoch 21, Loss: 9.128291130065918\n",
      "Epoch 22, Loss: 6.485597610473633\n",
      "Epoch 23, Loss: 6.59307336807251\n",
      "Epoch 24, Loss: 8.782209396362305\n",
      "Epoch 25, Loss: 6.58551549911499\n",
      "Epoch 26, Loss: 6.582289218902588\n",
      "Epoch 27, Loss: 7.564056396484375\n",
      "Epoch 28, Loss: 6.026889801025391\n",
      "Epoch 29, Loss: 4.86199951171875\n",
      "Epoch 30, Loss: 5.931609630584717\n",
      "Epoch 31, Loss: 4.105780124664307\n",
      "Epoch 32, Loss: 4.524569988250732\n",
      "Epoch 33, Loss: 5.404801368713379\n",
      "Epoch 34, Loss: 4.100687026977539\n",
      "Epoch 35, Loss: 2.6511425971984863\n",
      "Epoch 36, Loss: 4.185389041900635\n",
      "Epoch 37, Loss: 3.2872350215911865\n",
      "Epoch 38, Loss: 4.068057060241699\n",
      "Epoch 39, Loss: 3.8972489833831787\n",
      "Epoch 40, Loss: 2.4798097610473633\n",
      "Epoch 41, Loss: 3.0297300815582275\n",
      "Epoch 42, Loss: 2.87041974067688\n",
      "Epoch 43, Loss: 3.278531312942505\n",
      "Epoch 44, Loss: 2.8781001567840576\n",
      "Epoch 45, Loss: 2.83284592628479\n",
      "Epoch 46, Loss: 2.444021701812744\n",
      "Epoch 47, Loss: 2.696089506149292\n",
      "Epoch 48, Loss: 2.5639021396636963\n",
      "Epoch 49, Loss: 2.5753116607666016\n",
      "Epoch 50, Loss: 3.3239223957061768\n",
      "Epoch 51, Loss: 2.5846340656280518\n",
      "Epoch 52, Loss: 3.12101411819458\n",
      "Epoch 53, Loss: 2.324636459350586\n",
      "Epoch 54, Loss: 2.1524767875671387\n",
      "Epoch 55, Loss: 2.720414400100708\n",
      "Epoch 56, Loss: 2.283121347427368\n",
      "Epoch 57, Loss: 2.386077404022217\n",
      "Epoch 58, Loss: 2.18591046333313\n",
      "Epoch 59, Loss: 1.8441095352172852\n",
      "Epoch 60, Loss: 2.831268072128296\n",
      "Epoch 61, Loss: 2.262723922729492\n",
      "Epoch 62, Loss: 2.010648727416992\n",
      "Epoch 63, Loss: 3.0894365310668945\n",
      "Epoch 64, Loss: 2.4964184761047363\n",
      "Epoch 65, Loss: 2.3326728343963623\n",
      "Epoch 66, Loss: 1.9905314445495605\n",
      "Epoch 67, Loss: 1.9260847568511963\n",
      "Epoch 68, Loss: 2.100956916809082\n",
      "Epoch 69, Loss: 2.2610301971435547\n",
      "Epoch 70, Loss: 1.7427332401275635\n",
      "Epoch 71, Loss: 2.105213165283203\n",
      "Epoch 72, Loss: 2.5640974044799805\n",
      "Epoch 73, Loss: 1.8887782096862793\n",
      "Epoch 74, Loss: 2.16025972366333\n",
      "Epoch 75, Loss: 1.9790078401565552\n",
      "Epoch 76, Loss: 2.2269935607910156\n",
      "Epoch 77, Loss: 2.0311708450317383\n",
      "Epoch 78, Loss: 1.887698769569397\n",
      "Epoch 79, Loss: 1.8796772956848145\n",
      "Epoch 80, Loss: 2.5747382640838623\n",
      "Epoch 81, Loss: 1.8247406482696533\n",
      "Epoch 82, Loss: 2.073876142501831\n",
      "Epoch 83, Loss: 1.974470615386963\n",
      "Epoch 84, Loss: 1.7291532754898071\n",
      "Epoch 85, Loss: 1.9021658897399902\n",
      "Epoch 86, Loss: 1.9528979063034058\n",
      "Epoch 87, Loss: 1.9782321453094482\n",
      "Epoch 88, Loss: 1.7749015092849731\n",
      "Epoch 89, Loss: 1.8072352409362793\n",
      "Epoch 90, Loss: 1.8666777610778809\n",
      "Epoch 91, Loss: 1.7371699810028076\n",
      "Epoch 92, Loss: 1.8694167137145996\n",
      "Epoch 93, Loss: 1.9292951822280884\n",
      "Epoch 94, Loss: 1.981121301651001\n",
      "Epoch 95, Loss: 1.9904842376708984\n",
      "Epoch 96, Loss: 1.8744930028915405\n",
      "Epoch 97, Loss: 1.8282438516616821\n",
      "Epoch 98, Loss: 2.314547300338745\n",
      "Epoch 99, Loss: 2.0553536415100098\n",
      "Epoch 100, Loss: 1.820723056793213\n",
      "Validation Accuracy: 0.2698\n",
      "Epoch 1, Loss: 30.545495986938477\n",
      "Epoch 2, Loss: 21.936681747436523\n",
      "Epoch 3, Loss: 16.388532638549805\n",
      "Epoch 4, Loss: 16.537734985351562\n",
      "Epoch 5, Loss: 14.440363883972168\n",
      "Epoch 6, Loss: 14.207928657531738\n",
      "Epoch 7, Loss: 15.374099731445312\n",
      "Epoch 8, Loss: 13.945585250854492\n",
      "Epoch 9, Loss: 13.901409149169922\n",
      "Epoch 10, Loss: 8.788187026977539\n",
      "Epoch 11, Loss: 12.559467315673828\n",
      "Epoch 12, Loss: 8.5366792678833\n",
      "Epoch 13, Loss: 8.928079605102539\n",
      "Epoch 14, Loss: 6.758476257324219\n",
      "Epoch 15, Loss: 7.456883907318115\n",
      "Epoch 16, Loss: 7.654641151428223\n",
      "Epoch 17, Loss: 8.652510643005371\n",
      "Epoch 18, Loss: 8.956944465637207\n",
      "Epoch 19, Loss: 8.213770866394043\n",
      "Epoch 20, Loss: 7.012174606323242\n",
      "Epoch 21, Loss: 5.100316047668457\n",
      "Epoch 22, Loss: 6.648439407348633\n",
      "Epoch 23, Loss: 5.0247416496276855\n",
      "Epoch 24, Loss: 4.0653510093688965\n",
      "Epoch 25, Loss: 4.971415042877197\n",
      "Epoch 26, Loss: 3.9806265830993652\n",
      "Epoch 27, Loss: 4.604552268981934\n",
      "Epoch 28, Loss: 2.8169314861297607\n",
      "Epoch 29, Loss: 5.252935409545898\n",
      "Epoch 30, Loss: 3.124389886856079\n",
      "Epoch 31, Loss: 3.633518695831299\n",
      "Epoch 32, Loss: 2.864084482192993\n",
      "Epoch 33, Loss: 4.161957740783691\n",
      "Epoch 34, Loss: 3.4337706565856934\n",
      "Epoch 35, Loss: 3.5961484909057617\n",
      "Epoch 36, Loss: 4.270335674285889\n",
      "Epoch 37, Loss: 2.925990104675293\n",
      "Epoch 38, Loss: 3.12465500831604\n",
      "Epoch 39, Loss: 2.730945587158203\n",
      "Epoch 40, Loss: 2.5248913764953613\n",
      "Epoch 41, Loss: 2.449448823928833\n",
      "Epoch 42, Loss: 3.087477922439575\n",
      "Epoch 43, Loss: 2.069410562515259\n",
      "Epoch 44, Loss: 2.7965896129608154\n",
      "Epoch 45, Loss: 2.8091843128204346\n",
      "Epoch 46, Loss: 3.006612539291382\n",
      "Epoch 47, Loss: 2.31827712059021\n",
      "Epoch 48, Loss: 2.38820219039917\n",
      "Epoch 49, Loss: 2.212270736694336\n",
      "Epoch 50, Loss: 2.573431968688965\n",
      "Epoch 51, Loss: 2.49250864982605\n",
      "Epoch 52, Loss: 2.0368237495422363\n",
      "Epoch 53, Loss: 2.5342912673950195\n",
      "Epoch 54, Loss: 2.1935462951660156\n",
      "Epoch 55, Loss: 2.100637912750244\n",
      "Epoch 56, Loss: 2.271509885787964\n",
      "Epoch 57, Loss: 2.1625170707702637\n",
      "Epoch 58, Loss: 2.0492236614227295\n",
      "Epoch 59, Loss: 2.4308102130889893\n",
      "Epoch 60, Loss: 2.0044338703155518\n",
      "Epoch 61, Loss: 2.141967296600342\n",
      "Epoch 62, Loss: 1.6741268634796143\n",
      "Epoch 63, Loss: 2.625704288482666\n",
      "Epoch 64, Loss: 2.242274045944214\n",
      "Epoch 65, Loss: 2.1555910110473633\n",
      "Epoch 66, Loss: 2.1915276050567627\n",
      "Epoch 67, Loss: 1.7544995546340942\n",
      "Epoch 68, Loss: 2.30482816696167\n",
      "Epoch 69, Loss: 2.330382823944092\n",
      "Epoch 70, Loss: 1.9278501272201538\n",
      "Epoch 71, Loss: 1.7872538566589355\n",
      "Epoch 72, Loss: 2.106731653213501\n",
      "Epoch 73, Loss: 2.0493438243865967\n",
      "Epoch 74, Loss: 2.317760467529297\n",
      "Epoch 75, Loss: 1.911110520362854\n",
      "Epoch 76, Loss: 1.8573249578475952\n",
      "Epoch 77, Loss: 2.053544759750366\n",
      "Epoch 78, Loss: 1.8338087797164917\n",
      "Epoch 79, Loss: 1.7446714639663696\n",
      "Epoch 80, Loss: 1.9752099514007568\n",
      "Epoch 81, Loss: 1.8926693201065063\n",
      "Epoch 82, Loss: 1.809171438217163\n",
      "Epoch 83, Loss: 1.8080753087997437\n",
      "Epoch 84, Loss: 1.8448967933654785\n",
      "Epoch 85, Loss: 1.7249757051467896\n",
      "Epoch 86, Loss: 1.7500052452087402\n",
      "Epoch 87, Loss: 2.043065309524536\n",
      "Epoch 88, Loss: 1.890577793121338\n",
      "Epoch 89, Loss: 1.92754065990448\n",
      "Epoch 90, Loss: 1.8302606344223022\n",
      "Epoch 91, Loss: 2.1438326835632324\n",
      "Epoch 92, Loss: 1.9639006853103638\n",
      "Epoch 93, Loss: 1.9096184968948364\n",
      "Epoch 94, Loss: 1.9318801164627075\n",
      "Epoch 95, Loss: 2.1226718425750732\n",
      "Epoch 96, Loss: 2.1062071323394775\n",
      "Epoch 97, Loss: 1.6704360246658325\n",
      "Epoch 98, Loss: 1.937174916267395\n",
      "Epoch 99, Loss: 1.8300751447677612\n",
      "Epoch 100, Loss: 1.815687894821167\n",
      "Validation Accuracy: 0.2143\n",
      "Epoch 1, Loss: 12.012727737426758\n",
      "Epoch 2, Loss: 14.29606819152832\n",
      "Epoch 3, Loss: 11.54240608215332\n",
      "Epoch 4, Loss: 12.175188064575195\n",
      "Epoch 5, Loss: 12.290283203125\n",
      "Epoch 6, Loss: 10.344595909118652\n",
      "Epoch 7, Loss: 8.852982521057129\n",
      "Epoch 8, Loss: 10.663650512695312\n",
      "Epoch 9, Loss: 8.31265926361084\n",
      "Epoch 10, Loss: 7.332424640655518\n",
      "Epoch 11, Loss: 7.285507678985596\n",
      "Epoch 12, Loss: 9.146583557128906\n",
      "Epoch 13, Loss: 6.636533737182617\n",
      "Epoch 14, Loss: 5.899320602416992\n",
      "Epoch 15, Loss: 5.8604326248168945\n",
      "Epoch 16, Loss: 4.8988237380981445\n",
      "Epoch 17, Loss: 5.211410045623779\n",
      "Epoch 18, Loss: 5.040475845336914\n",
      "Epoch 19, Loss: 5.642467021942139\n",
      "Epoch 20, Loss: 4.672473430633545\n",
      "Epoch 21, Loss: 4.413029670715332\n",
      "Epoch 22, Loss: 3.6398587226867676\n",
      "Epoch 23, Loss: 3.2778913974761963\n",
      "Epoch 24, Loss: 3.9841372966766357\n",
      "Epoch 25, Loss: 3.4415738582611084\n",
      "Epoch 26, Loss: 3.7472891807556152\n",
      "Epoch 27, Loss: 3.396634578704834\n",
      "Epoch 28, Loss: 2.9112582206726074\n",
      "Epoch 29, Loss: 3.082357168197632\n",
      "Epoch 30, Loss: 2.687955856323242\n",
      "Epoch 31, Loss: 2.6391308307647705\n",
      "Epoch 32, Loss: 2.7039880752563477\n",
      "Epoch 33, Loss: 2.3116767406463623\n",
      "Epoch 34, Loss: 3.1178321838378906\n",
      "Epoch 35, Loss: 2.243159770965576\n",
      "Epoch 36, Loss: 2.344053268432617\n",
      "Epoch 37, Loss: 2.5540966987609863\n",
      "Epoch 38, Loss: 2.689608097076416\n",
      "Epoch 39, Loss: 2.878495216369629\n",
      "Epoch 40, Loss: 2.2206437587738037\n",
      "Epoch 41, Loss: 1.8489630222320557\n",
      "Epoch 42, Loss: 1.9481914043426514\n",
      "Epoch 43, Loss: 2.489349365234375\n",
      "Epoch 44, Loss: 2.3463516235351562\n",
      "Epoch 45, Loss: 1.983869194984436\n",
      "Epoch 46, Loss: 2.110560655593872\n",
      "Epoch 47, Loss: 2.116135358810425\n",
      "Epoch 48, Loss: 2.2779622077941895\n",
      "Epoch 49, Loss: 2.1635403633117676\n",
      "Epoch 50, Loss: 2.0007340908050537\n",
      "Epoch 51, Loss: 1.93925940990448\n",
      "Epoch 52, Loss: 2.0168120861053467\n",
      "Epoch 53, Loss: 2.146339178085327\n",
      "Epoch 54, Loss: 1.797608733177185\n",
      "Epoch 55, Loss: 2.101287841796875\n",
      "Epoch 56, Loss: 2.076343059539795\n",
      "Epoch 57, Loss: 1.7759873867034912\n",
      "Epoch 58, Loss: 1.936301350593567\n",
      "Epoch 59, Loss: 1.8773616552352905\n",
      "Epoch 60, Loss: 1.9282387495040894\n",
      "Epoch 61, Loss: 2.0511486530303955\n",
      "Epoch 62, Loss: 2.0025253295898438\n",
      "Epoch 63, Loss: 2.126243829727173\n",
      "Epoch 64, Loss: 2.0235190391540527\n",
      "Epoch 65, Loss: 2.24359130859375\n",
      "Epoch 66, Loss: 1.9739820957183838\n",
      "Epoch 67, Loss: 1.9575997591018677\n",
      "Epoch 68, Loss: 2.0023481845855713\n",
      "Epoch 69, Loss: 1.9796161651611328\n",
      "Epoch 70, Loss: 1.9376153945922852\n",
      "Epoch 71, Loss: 1.8151798248291016\n",
      "Epoch 72, Loss: 2.093987226486206\n",
      "Epoch 73, Loss: 1.8342150449752808\n",
      "Epoch 74, Loss: 1.6755205392837524\n",
      "Epoch 75, Loss: 1.986181378364563\n",
      "Epoch 76, Loss: 1.8545444011688232\n",
      "Epoch 77, Loss: 1.8890045881271362\n",
      "Epoch 78, Loss: 1.8610365390777588\n",
      "Epoch 79, Loss: 1.6459952592849731\n",
      "Epoch 80, Loss: 1.6974536180496216\n",
      "Epoch 81, Loss: 1.8141318559646606\n",
      "Epoch 82, Loss: 1.9674359560012817\n",
      "Epoch 83, Loss: 1.8081482648849487\n",
      "Epoch 84, Loss: 1.8712252378463745\n",
      "Epoch 85, Loss: 1.7674258947372437\n",
      "Epoch 86, Loss: 1.7021137475967407\n",
      "Epoch 87, Loss: 1.976269006729126\n",
      "Epoch 88, Loss: 2.0547568798065186\n",
      "Epoch 89, Loss: 1.9109160900115967\n",
      "Epoch 90, Loss: 1.8332980871200562\n",
      "Epoch 91, Loss: 1.588685393333435\n",
      "Epoch 92, Loss: 1.722488284111023\n",
      "Epoch 93, Loss: 1.8857399225234985\n",
      "Epoch 94, Loss: 2.003652572631836\n",
      "Epoch 95, Loss: 1.860506534576416\n",
      "Epoch 96, Loss: 1.7693098783493042\n",
      "Epoch 97, Loss: 2.0620481967926025\n",
      "Epoch 98, Loss: 1.9351967573165894\n",
      "Epoch 99, Loss: 1.833012342453003\n",
      "Epoch 100, Loss: 1.7437278032302856\n",
      "Validation Accuracy: 0.2407\n",
      "Epoch 1, Loss: 12.043736457824707\n",
      "Epoch 2, Loss: 7.3965935707092285\n",
      "Epoch 3, Loss: 7.210638046264648\n",
      "Epoch 4, Loss: 9.2034273147583\n",
      "Epoch 5, Loss: 8.156961441040039\n",
      "Epoch 6, Loss: 7.125725269317627\n",
      "Epoch 7, Loss: 6.7214035987854\n",
      "Epoch 8, Loss: 5.485657215118408\n",
      "Epoch 9, Loss: 6.895441055297852\n",
      "Epoch 10, Loss: 4.3811540603637695\n",
      "Epoch 11, Loss: 4.986701011657715\n",
      "Epoch 12, Loss: 3.8016321659088135\n",
      "Epoch 13, Loss: 4.695059299468994\n",
      "Epoch 14, Loss: 5.236709117889404\n",
      "Epoch 15, Loss: 3.9403014183044434\n",
      "Epoch 16, Loss: 3.6319985389709473\n",
      "Epoch 17, Loss: 3.4043338298797607\n",
      "Epoch 18, Loss: 3.6957342624664307\n",
      "Epoch 19, Loss: 2.72951340675354\n",
      "Epoch 20, Loss: 2.8863821029663086\n",
      "Epoch 21, Loss: 2.619647979736328\n",
      "Epoch 22, Loss: 2.689918279647827\n",
      "Epoch 23, Loss: 2.5083863735198975\n",
      "Epoch 24, Loss: 2.8544490337371826\n",
      "Epoch 25, Loss: 2.935636043548584\n",
      "Epoch 26, Loss: 2.904177188873291\n",
      "Epoch 27, Loss: 2.9386119842529297\n",
      "Epoch 28, Loss: 2.2943742275238037\n",
      "Epoch 29, Loss: 2.892134428024292\n",
      "Epoch 30, Loss: 2.7974016666412354\n",
      "Epoch 31, Loss: 2.7170135974884033\n",
      "Epoch 32, Loss: 1.9929288625717163\n",
      "Epoch 33, Loss: 2.5629425048828125\n",
      "Epoch 34, Loss: 2.2529172897338867\n",
      "Epoch 35, Loss: 1.929100513458252\n",
      "Epoch 36, Loss: 2.2205443382263184\n",
      "Epoch 37, Loss: 1.9304556846618652\n",
      "Epoch 38, Loss: 1.8559943437576294\n",
      "Epoch 39, Loss: 2.083218574523926\n",
      "Epoch 40, Loss: 1.733059048652649\n",
      "Epoch 41, Loss: 1.9108539819717407\n",
      "Epoch 42, Loss: 1.940144419670105\n",
      "Epoch 43, Loss: 2.0730180740356445\n",
      "Epoch 44, Loss: 2.1361148357391357\n",
      "Epoch 45, Loss: 2.1448118686676025\n",
      "Epoch 46, Loss: 1.8406559228897095\n",
      "Epoch 47, Loss: 1.957876205444336\n",
      "Epoch 48, Loss: 2.24312686920166\n",
      "Epoch 49, Loss: 1.6708333492279053\n",
      "Epoch 50, Loss: 1.8656893968582153\n",
      "Epoch 51, Loss: 1.9775279760360718\n",
      "Epoch 52, Loss: 1.679075002670288\n",
      "Epoch 53, Loss: 1.6646454334259033\n",
      "Epoch 54, Loss: 1.700303077697754\n",
      "Epoch 55, Loss: 1.9049714803695679\n",
      "Epoch 56, Loss: 1.9497828483581543\n",
      "Epoch 57, Loss: 2.055194616317749\n",
      "Epoch 58, Loss: 1.8355565071105957\n",
      "Epoch 59, Loss: 1.6054890155792236\n",
      "Epoch 60, Loss: 1.887986421585083\n",
      "Epoch 61, Loss: 1.7412917613983154\n",
      "Epoch 62, Loss: 1.8561656475067139\n",
      "Epoch 63, Loss: 1.8632166385650635\n",
      "Epoch 64, Loss: 1.8301830291748047\n",
      "Epoch 65, Loss: 1.7059617042541504\n",
      "Epoch 66, Loss: 1.6234036684036255\n",
      "Epoch 67, Loss: 1.4613277912139893\n",
      "Epoch 68, Loss: 2.2130351066589355\n",
      "Epoch 69, Loss: 1.8360143899917603\n",
      "Epoch 70, Loss: 1.5743615627288818\n",
      "Epoch 71, Loss: 1.973912000656128\n",
      "Epoch 72, Loss: 1.5243242979049683\n",
      "Epoch 73, Loss: 1.7483409643173218\n",
      "Epoch 74, Loss: 1.6228677034378052\n",
      "Epoch 75, Loss: 1.6488295793533325\n",
      "Epoch 76, Loss: 1.9784857034683228\n",
      "Epoch 77, Loss: 1.943037986755371\n",
      "Epoch 78, Loss: 1.657296895980835\n",
      "Epoch 79, Loss: 1.9740219116210938\n",
      "Epoch 80, Loss: 1.655239224433899\n",
      "Epoch 81, Loss: 1.6179598569869995\n",
      "Epoch 82, Loss: 1.6241750717163086\n",
      "Epoch 83, Loss: 1.4964128732681274\n",
      "Epoch 84, Loss: 1.8478052616119385\n",
      "Epoch 85, Loss: 1.733522653579712\n",
      "Epoch 86, Loss: 1.798109769821167\n",
      "Epoch 87, Loss: 1.6168662309646606\n",
      "Epoch 88, Loss: 1.5794633626937866\n",
      "Epoch 89, Loss: 1.5040807723999023\n",
      "Epoch 90, Loss: 1.7893491983413696\n",
      "Epoch 91, Loss: 1.6773006916046143\n",
      "Epoch 92, Loss: 1.5901552438735962\n",
      "Epoch 93, Loss: 1.723915696144104\n",
      "Epoch 94, Loss: 1.6805095672607422\n",
      "Epoch 95, Loss: 1.6795343160629272\n",
      "Epoch 96, Loss: 1.514731764793396\n",
      "Epoch 97, Loss: 1.87954580783844\n",
      "Epoch 98, Loss: 1.925836443901062\n",
      "Epoch 99, Loss: 1.7373404502868652\n",
      "Epoch 100, Loss: 1.5923281908035278\n",
      "Validation Accuracy: 0.3360\n",
      "Epoch 1, Loss: 9.40172004699707\n",
      "Epoch 2, Loss: 8.52721881866455\n",
      "Epoch 3, Loss: 8.043437004089355\n",
      "Epoch 4, Loss: 7.481319904327393\n",
      "Epoch 5, Loss: 6.503157615661621\n",
      "Epoch 6, Loss: 4.759603500366211\n",
      "Epoch 7, Loss: 5.784590244293213\n",
      "Epoch 8, Loss: 4.8740153312683105\n",
      "Epoch 9, Loss: 4.762680530548096\n",
      "Epoch 10, Loss: 4.213104724884033\n",
      "Epoch 11, Loss: 5.448318004608154\n",
      "Epoch 12, Loss: 4.88969087600708\n",
      "Epoch 13, Loss: 4.189568996429443\n",
      "Epoch 14, Loss: 3.784285545349121\n",
      "Epoch 15, Loss: 3.598989725112915\n",
      "Epoch 16, Loss: 3.428536891937256\n",
      "Epoch 17, Loss: 4.2037763595581055\n",
      "Epoch 18, Loss: 3.0934078693389893\n",
      "Epoch 19, Loss: 3.5127992630004883\n",
      "Epoch 20, Loss: 2.3482630252838135\n",
      "Epoch 21, Loss: 3.2016215324401855\n",
      "Epoch 22, Loss: 2.9988770484924316\n",
      "Epoch 23, Loss: 2.9161617755889893\n",
      "Epoch 24, Loss: 2.549605131149292\n",
      "Epoch 25, Loss: 2.1863181591033936\n",
      "Epoch 26, Loss: 1.834679126739502\n",
      "Epoch 27, Loss: 2.0162551403045654\n",
      "Epoch 28, Loss: 2.089188814163208\n",
      "Epoch 29, Loss: 2.053060531616211\n",
      "Epoch 30, Loss: 2.3399102687835693\n",
      "Epoch 31, Loss: 2.3016178607940674\n",
      "Epoch 32, Loss: 2.3864738941192627\n",
      "Epoch 33, Loss: 2.0453906059265137\n",
      "Epoch 34, Loss: 1.6829631328582764\n",
      "Epoch 35, Loss: 2.3327653408050537\n",
      "Epoch 36, Loss: 1.7412420511245728\n",
      "Epoch 37, Loss: 1.9948949813842773\n",
      "Epoch 38, Loss: 1.9238823652267456\n",
      "Epoch 39, Loss: 1.8772175312042236\n",
      "Epoch 40, Loss: 2.445725440979004\n",
      "Epoch 41, Loss: 2.360633134841919\n",
      "Epoch 42, Loss: 1.6524194478988647\n",
      "Epoch 43, Loss: 2.262671709060669\n",
      "Epoch 44, Loss: 1.9794284105300903\n",
      "Epoch 45, Loss: 2.0084280967712402\n",
      "Epoch 46, Loss: 1.9983915090560913\n",
      "Epoch 47, Loss: 1.8053323030471802\n",
      "Epoch 48, Loss: 1.8374103307724\n",
      "Epoch 49, Loss: 1.895882248878479\n",
      "Epoch 50, Loss: 1.7897652387619019\n",
      "Epoch 51, Loss: 1.8492975234985352\n",
      "Epoch 52, Loss: 1.7490010261535645\n",
      "Epoch 53, Loss: 1.7765549421310425\n",
      "Epoch 54, Loss: 1.4770866632461548\n",
      "Epoch 55, Loss: 1.8238450288772583\n",
      "Epoch 56, Loss: 1.6886831521987915\n",
      "Epoch 57, Loss: 1.9615483283996582\n",
      "Epoch 58, Loss: 1.492362141609192\n",
      "Epoch 59, Loss: 1.916455864906311\n",
      "Epoch 60, Loss: 1.9359705448150635\n",
      "Epoch 61, Loss: 1.8348811864852905\n",
      "Epoch 62, Loss: 1.6306004524230957\n",
      "Epoch 63, Loss: 1.8862788677215576\n",
      "Epoch 64, Loss: 1.606123924255371\n",
      "Epoch 65, Loss: 1.524442434310913\n",
      "Epoch 66, Loss: 1.68085515499115\n",
      "Epoch 67, Loss: 1.882861852645874\n",
      "Epoch 68, Loss: 1.7517777681350708\n",
      "Epoch 69, Loss: 2.056690216064453\n",
      "Epoch 70, Loss: 1.784303069114685\n",
      "Epoch 71, Loss: 1.8622626066207886\n",
      "Epoch 72, Loss: 1.9554990530014038\n",
      "Epoch 73, Loss: 1.581856608390808\n",
      "Epoch 74, Loss: 1.8704110383987427\n",
      "Epoch 75, Loss: 1.6386654376983643\n",
      "Epoch 76, Loss: 1.6183470487594604\n",
      "Epoch 77, Loss: 1.8535397052764893\n",
      "Epoch 78, Loss: 1.590498685836792\n",
      "Epoch 79, Loss: 1.6127803325653076\n",
      "Epoch 80, Loss: 1.6256866455078125\n",
      "Epoch 81, Loss: 1.8940428495407104\n",
      "Epoch 82, Loss: 1.6805338859558105\n",
      "Epoch 83, Loss: 1.5573439598083496\n",
      "Epoch 84, Loss: 1.7766364812850952\n",
      "Epoch 85, Loss: 1.56775963306427\n",
      "Epoch 86, Loss: 1.6472914218902588\n",
      "Epoch 87, Loss: 1.6279749870300293\n",
      "Epoch 88, Loss: 1.7843493223190308\n",
      "Epoch 89, Loss: 1.4495863914489746\n",
      "Epoch 90, Loss: 1.6564769744873047\n",
      "Epoch 91, Loss: 1.4906679391860962\n",
      "Epoch 92, Loss: 1.517041802406311\n",
      "Epoch 93, Loss: 1.5471657514572144\n",
      "Epoch 94, Loss: 1.6995742321014404\n",
      "Epoch 95, Loss: 1.5210741758346558\n",
      "Epoch 96, Loss: 1.4629237651824951\n",
      "Epoch 97, Loss: 1.4642760753631592\n",
      "Epoch 98, Loss: 1.6780211925506592\n",
      "Epoch 99, Loss: 1.4276353120803833\n",
      "Epoch 100, Loss: 1.7670207023620605\n",
      "Validation Accuracy: 0.3492\n",
      "Epoch 1, Loss: 8.638555526733398\n",
      "Epoch 2, Loss: 5.805469989776611\n",
      "Epoch 3, Loss: 5.463504314422607\n",
      "Epoch 4, Loss: 6.28971529006958\n",
      "Epoch 5, Loss: 5.966710567474365\n",
      "Epoch 6, Loss: 5.741518020629883\n",
      "Epoch 7, Loss: 5.698770046234131\n",
      "Epoch 8, Loss: 5.127856731414795\n",
      "Epoch 9, Loss: 4.387380599975586\n",
      "Epoch 10, Loss: 4.280651569366455\n",
      "Epoch 11, Loss: 4.20070743560791\n",
      "Epoch 12, Loss: 4.418804168701172\n",
      "Epoch 13, Loss: 3.7463154792785645\n",
      "Epoch 14, Loss: 3.4457552433013916\n",
      "Epoch 15, Loss: 4.145490646362305\n",
      "Epoch 16, Loss: 2.829145669937134\n",
      "Epoch 17, Loss: 3.3811240196228027\n",
      "Epoch 18, Loss: 2.803943395614624\n",
      "Epoch 19, Loss: 3.1379754543304443\n",
      "Epoch 20, Loss: 3.2030327320098877\n",
      "Epoch 21, Loss: 2.285400867462158\n",
      "Epoch 22, Loss: 2.3811371326446533\n",
      "Epoch 23, Loss: 2.3165030479431152\n",
      "Epoch 24, Loss: 2.4821557998657227\n",
      "Epoch 25, Loss: 2.5521936416625977\n",
      "Epoch 26, Loss: 2.077336311340332\n",
      "Epoch 27, Loss: 2.584176778793335\n",
      "Epoch 28, Loss: 2.1746394634246826\n",
      "Epoch 29, Loss: 2.211604118347168\n",
      "Epoch 30, Loss: 2.1580915451049805\n",
      "Epoch 31, Loss: 2.2040326595306396\n",
      "Epoch 32, Loss: 2.153052806854248\n",
      "Epoch 33, Loss: 1.898573398590088\n",
      "Epoch 34, Loss: 1.934065818786621\n",
      "Epoch 35, Loss: 2.080843925476074\n",
      "Epoch 36, Loss: 2.064906597137451\n",
      "Epoch 37, Loss: 1.5751301050186157\n",
      "Epoch 38, Loss: 2.2710509300231934\n",
      "Epoch 39, Loss: 2.1474225521087646\n",
      "Epoch 40, Loss: 1.937946081161499\n",
      "Epoch 41, Loss: 1.705236554145813\n",
      "Epoch 42, Loss: 1.5741289854049683\n",
      "Epoch 43, Loss: 1.552260398864746\n",
      "Epoch 44, Loss: 1.878514051437378\n",
      "Epoch 45, Loss: 1.8141618967056274\n",
      "Epoch 46, Loss: 1.648056149482727\n",
      "Epoch 47, Loss: 1.6571322679519653\n",
      "Epoch 48, Loss: 1.6384838819503784\n",
      "Epoch 49, Loss: 1.8382841348648071\n",
      "Epoch 50, Loss: 1.583133578300476\n",
      "Epoch 51, Loss: 1.636563777923584\n",
      "Epoch 52, Loss: 1.796500325202942\n",
      "Epoch 53, Loss: 1.7641202211380005\n",
      "Epoch 54, Loss: 1.805249810218811\n",
      "Epoch 55, Loss: 1.7430334091186523\n",
      "Epoch 56, Loss: 1.9424235820770264\n",
      "Epoch 57, Loss: 1.7097221612930298\n",
      "Epoch 58, Loss: 1.7185825109481812\n",
      "Epoch 59, Loss: 1.9475406408309937\n",
      "Epoch 60, Loss: 1.9310367107391357\n",
      "Epoch 61, Loss: 1.5548384189605713\n",
      "Epoch 62, Loss: 1.8643543720245361\n",
      "Epoch 63, Loss: 1.835276484489441\n",
      "Epoch 64, Loss: 1.8458009958267212\n",
      "Epoch 65, Loss: 1.7791093587875366\n",
      "Epoch 66, Loss: 1.6487767696380615\n",
      "Epoch 67, Loss: 1.9166518449783325\n",
      "Epoch 68, Loss: 1.331681489944458\n",
      "Epoch 69, Loss: 1.4714829921722412\n",
      "Epoch 70, Loss: 1.4175833463668823\n",
      "Epoch 71, Loss: 1.468514323234558\n",
      "Epoch 72, Loss: 1.5295827388763428\n",
      "Epoch 73, Loss: 1.5541784763336182\n",
      "Epoch 74, Loss: 1.8002959489822388\n",
      "Epoch 75, Loss: 1.5231494903564453\n",
      "Epoch 76, Loss: 1.6258256435394287\n",
      "Epoch 77, Loss: 1.6769403219223022\n",
      "Epoch 78, Loss: 1.6780143976211548\n",
      "Epoch 79, Loss: 1.6683777570724487\n",
      "Epoch 80, Loss: 1.4991940259933472\n",
      "Epoch 81, Loss: 1.5837777853012085\n",
      "Epoch 82, Loss: 1.4885809421539307\n",
      "Epoch 83, Loss: 1.572538137435913\n",
      "Epoch 84, Loss: 1.538090467453003\n",
      "Epoch 85, Loss: 1.591745376586914\n",
      "Epoch 86, Loss: 1.4769927263259888\n",
      "Epoch 87, Loss: 1.5946719646453857\n",
      "Epoch 88, Loss: 1.7012284994125366\n",
      "Epoch 89, Loss: 1.722682237625122\n",
      "Epoch 90, Loss: 1.6956759691238403\n",
      "Epoch 91, Loss: 1.4148482084274292\n",
      "Epoch 92, Loss: 1.4849960803985596\n",
      "Epoch 93, Loss: 1.3777573108673096\n",
      "Epoch 94, Loss: 1.4925299882888794\n",
      "Epoch 95, Loss: 1.661074161529541\n",
      "Epoch 96, Loss: 1.4783918857574463\n",
      "Epoch 97, Loss: 1.7046765089035034\n",
      "Epoch 98, Loss: 1.5139120817184448\n",
      "Epoch 99, Loss: 1.645578384399414\n",
      "Epoch 100, Loss: 1.2669440507888794\n",
      "Validation Accuracy: 0.3333\n",
      "Epoch 1, Loss: 5.579458236694336\n",
      "Epoch 2, Loss: 6.0992021560668945\n",
      "Epoch 3, Loss: 6.139233112335205\n",
      "Epoch 4, Loss: 5.63244104385376\n",
      "Epoch 5, Loss: 4.05636739730835\n",
      "Epoch 6, Loss: 4.226372718811035\n",
      "Epoch 7, Loss: 4.4629621505737305\n",
      "Epoch 8, Loss: 4.807332992553711\n",
      "Epoch 9, Loss: 4.12677526473999\n",
      "Epoch 10, Loss: 3.5182507038116455\n",
      "Epoch 11, Loss: 3.6549718379974365\n",
      "Epoch 12, Loss: 3.673067092895508\n",
      "Epoch 13, Loss: 3.478207588195801\n",
      "Epoch 14, Loss: 2.776982069015503\n",
      "Epoch 15, Loss: 2.5439260005950928\n",
      "Epoch 16, Loss: 2.5806312561035156\n",
      "Epoch 17, Loss: 3.005018472671509\n",
      "Epoch 18, Loss: 2.291208028793335\n",
      "Epoch 19, Loss: 1.906838059425354\n",
      "Epoch 20, Loss: 2.8856728076934814\n",
      "Epoch 21, Loss: 3.1438803672790527\n",
      "Epoch 22, Loss: 1.8497834205627441\n",
      "Epoch 23, Loss: 2.2334718704223633\n",
      "Epoch 24, Loss: 2.014289140701294\n",
      "Epoch 25, Loss: 2.147404670715332\n",
      "Epoch 26, Loss: 2.125023365020752\n",
      "Epoch 27, Loss: 1.9528069496154785\n",
      "Epoch 28, Loss: 1.4982914924621582\n",
      "Epoch 29, Loss: 1.721872091293335\n",
      "Epoch 30, Loss: 2.2761502265930176\n",
      "Epoch 31, Loss: 1.9325588941574097\n",
      "Epoch 32, Loss: 1.9687501192092896\n",
      "Epoch 33, Loss: 1.721030592918396\n",
      "Epoch 34, Loss: 1.6933729648590088\n",
      "Epoch 35, Loss: 1.8048235177993774\n",
      "Epoch 36, Loss: 1.5649749040603638\n",
      "Epoch 37, Loss: 1.6905157566070557\n",
      "Epoch 38, Loss: 1.570461630821228\n",
      "Epoch 39, Loss: 1.7272566556930542\n",
      "Epoch 40, Loss: 1.7834056615829468\n",
      "Epoch 41, Loss: 1.8616303205490112\n",
      "Epoch 42, Loss: 1.7132571935653687\n",
      "Epoch 43, Loss: 1.7325363159179688\n",
      "Epoch 44, Loss: 1.9125491380691528\n",
      "Epoch 45, Loss: 2.2322635650634766\n",
      "Epoch 46, Loss: 1.715134620666504\n",
      "Epoch 47, Loss: 1.605118751525879\n",
      "Epoch 48, Loss: 1.8566631078720093\n",
      "Epoch 49, Loss: 1.9894553422927856\n",
      "Epoch 50, Loss: 1.4811979532241821\n",
      "Epoch 51, Loss: 1.5443905591964722\n",
      "Epoch 52, Loss: 1.869166612625122\n",
      "Epoch 53, Loss: 1.5413050651550293\n",
      "Epoch 54, Loss: 1.644570231437683\n",
      "Epoch 55, Loss: 1.5310568809509277\n",
      "Epoch 56, Loss: 1.7182127237319946\n",
      "Epoch 57, Loss: 1.6365913152694702\n",
      "Epoch 58, Loss: 1.6257179975509644\n",
      "Epoch 59, Loss: 1.674978256225586\n",
      "Epoch 60, Loss: 1.656024694442749\n",
      "Epoch 61, Loss: 1.7107222080230713\n",
      "Epoch 62, Loss: 1.5715606212615967\n",
      "Epoch 63, Loss: 1.4778931140899658\n",
      "Epoch 64, Loss: 1.750163197517395\n",
      "Epoch 65, Loss: 1.4851019382476807\n",
      "Epoch 66, Loss: 1.4457257986068726\n",
      "Epoch 67, Loss: 1.6997729539871216\n",
      "Epoch 68, Loss: 1.3803699016571045\n",
      "Epoch 69, Loss: 1.5177472829818726\n",
      "Epoch 70, Loss: 1.527133822441101\n",
      "Epoch 71, Loss: 1.4231454133987427\n",
      "Epoch 72, Loss: 1.565116047859192\n",
      "Epoch 73, Loss: 1.4767173528671265\n",
      "Epoch 74, Loss: 1.4684348106384277\n",
      "Epoch 75, Loss: 1.7303260564804077\n",
      "Epoch 76, Loss: 1.3745404481887817\n",
      "Epoch 77, Loss: 1.5038856267929077\n",
      "Epoch 78, Loss: 1.7463454008102417\n",
      "Epoch 79, Loss: 1.543377161026001\n",
      "Epoch 80, Loss: 1.5135867595672607\n",
      "Epoch 81, Loss: 1.452056884765625\n",
      "Epoch 82, Loss: 1.3939002752304077\n",
      "Epoch 83, Loss: 1.4563754796981812\n",
      "Epoch 84, Loss: 1.5417497158050537\n",
      "Epoch 85, Loss: 1.47197687625885\n",
      "Epoch 86, Loss: 1.4527575969696045\n",
      "Epoch 87, Loss: 1.339643120765686\n",
      "Epoch 88, Loss: 1.396770715713501\n",
      "Epoch 89, Loss: 1.463630199432373\n",
      "Epoch 90, Loss: 1.417293667793274\n",
      "Epoch 91, Loss: 1.625901699066162\n",
      "Epoch 92, Loss: 1.447333812713623\n",
      "Epoch 93, Loss: 1.4241646528244019\n",
      "Epoch 94, Loss: 1.2574158906936646\n",
      "Epoch 95, Loss: 1.5913745164871216\n",
      "Epoch 96, Loss: 1.2304257154464722\n",
      "Epoch 97, Loss: 1.2243808507919312\n",
      "Epoch 98, Loss: 1.2700961828231812\n",
      "Epoch 99, Loss: 1.5412266254425049\n",
      "Epoch 100, Loss: 1.3143887519836426\n",
      "Validation Accuracy: 0.3862\n",
      "Epoch 1, Loss: 6.391619682312012\n",
      "Epoch 2, Loss: 5.480923175811768\n",
      "Epoch 3, Loss: 5.825284957885742\n",
      "Epoch 4, Loss: 4.589540958404541\n",
      "Epoch 5, Loss: 4.745245456695557\n",
      "Epoch 6, Loss: 5.0510687828063965\n",
      "Epoch 7, Loss: 3.357861042022705\n",
      "Epoch 8, Loss: 3.974714517593384\n",
      "Epoch 9, Loss: 3.369208574295044\n",
      "Epoch 10, Loss: 3.5837204456329346\n",
      "Epoch 11, Loss: 3.429617404937744\n",
      "Epoch 12, Loss: 3.01692271232605\n",
      "Epoch 13, Loss: 2.6498184204101562\n",
      "Epoch 14, Loss: 2.836256980895996\n",
      "Epoch 15, Loss: 3.233473777770996\n",
      "Epoch 16, Loss: 2.8368027210235596\n",
      "Epoch 17, Loss: 2.8315377235412598\n",
      "Epoch 18, Loss: 2.5637340545654297\n",
      "Epoch 19, Loss: 2.6827149391174316\n",
      "Epoch 20, Loss: 2.5758495330810547\n",
      "Epoch 21, Loss: 2.2180352210998535\n",
      "Epoch 22, Loss: 2.4412829875946045\n",
      "Epoch 23, Loss: 1.8289449214935303\n",
      "Epoch 24, Loss: 1.9043771028518677\n",
      "Epoch 25, Loss: 2.0547640323638916\n",
      "Epoch 26, Loss: 1.8130533695220947\n",
      "Epoch 27, Loss: 2.166956901550293\n",
      "Epoch 28, Loss: 2.0865771770477295\n",
      "Epoch 29, Loss: 2.04178524017334\n",
      "Epoch 30, Loss: 2.1627542972564697\n",
      "Epoch 31, Loss: 1.910549283027649\n",
      "Epoch 32, Loss: 1.8258284330368042\n",
      "Epoch 33, Loss: 1.9520994424819946\n",
      "Epoch 34, Loss: 1.7072200775146484\n",
      "Epoch 35, Loss: 1.890058159828186\n",
      "Epoch 36, Loss: 1.4313825368881226\n",
      "Epoch 37, Loss: 1.703432559967041\n",
      "Epoch 38, Loss: 1.828896403312683\n",
      "Epoch 39, Loss: 1.7754961252212524\n",
      "Epoch 40, Loss: 1.7263519763946533\n",
      "Epoch 41, Loss: 1.8894113302230835\n",
      "Epoch 42, Loss: 1.7582025527954102\n",
      "Epoch 43, Loss: 2.1613106727600098\n",
      "Epoch 44, Loss: 1.7564960718154907\n",
      "Epoch 45, Loss: 1.6185051202774048\n",
      "Epoch 46, Loss: 1.4406545162200928\n",
      "Epoch 47, Loss: 1.5996954441070557\n",
      "Epoch 48, Loss: 1.4325997829437256\n",
      "Epoch 49, Loss: 1.3139522075653076\n",
      "Epoch 50, Loss: 1.525067925453186\n",
      "Epoch 51, Loss: 1.3024885654449463\n",
      "Epoch 52, Loss: 1.5002514123916626\n",
      "Epoch 53, Loss: 1.4337255954742432\n",
      "Epoch 54, Loss: 1.6644155979156494\n",
      "Epoch 55, Loss: 1.532390832901001\n",
      "Epoch 56, Loss: 1.560571551322937\n",
      "Epoch 57, Loss: 1.824082612991333\n",
      "Epoch 58, Loss: 1.750545859336853\n",
      "Epoch 59, Loss: 1.6939269304275513\n",
      "Epoch 60, Loss: 1.3756821155548096\n",
      "Epoch 61, Loss: 1.3888410329818726\n",
      "Epoch 62, Loss: 1.8356852531433105\n",
      "Epoch 63, Loss: 1.3388724327087402\n",
      "Epoch 64, Loss: 1.4080750942230225\n",
      "Epoch 65, Loss: 1.4936739206314087\n",
      "Epoch 66, Loss: 1.5657639503479004\n",
      "Epoch 67, Loss: 1.2604361772537231\n",
      "Epoch 68, Loss: 1.6247540712356567\n",
      "Epoch 69, Loss: 1.55121910572052\n",
      "Epoch 70, Loss: 1.3724998235702515\n",
      "Epoch 71, Loss: 1.4227550029754639\n",
      "Epoch 72, Loss: 1.2738702297210693\n",
      "Epoch 73, Loss: 1.3531990051269531\n",
      "Epoch 74, Loss: 1.4817137718200684\n",
      "Epoch 75, Loss: 1.4667822122573853\n",
      "Epoch 76, Loss: 1.542941927909851\n",
      "Epoch 77, Loss: 1.5594482421875\n",
      "Epoch 78, Loss: 1.5882587432861328\n",
      "Epoch 79, Loss: 1.3682173490524292\n",
      "Epoch 80, Loss: 1.4534621238708496\n",
      "Epoch 81, Loss: 1.4504822492599487\n",
      "Epoch 82, Loss: 1.3430484533309937\n",
      "Epoch 83, Loss: 1.3665839433670044\n",
      "Epoch 84, Loss: 1.4532897472381592\n",
      "Epoch 85, Loss: 1.5080567598342896\n",
      "Epoch 86, Loss: 1.3856881856918335\n",
      "Epoch 87, Loss: 1.2607489824295044\n",
      "Epoch 88, Loss: 1.327018141746521\n",
      "Epoch 89, Loss: 1.212898850440979\n",
      "Epoch 90, Loss: 1.119025707244873\n",
      "Epoch 91, Loss: 1.359234094619751\n",
      "Epoch 92, Loss: 1.1228584051132202\n",
      "Epoch 93, Loss: 1.294956088066101\n",
      "Epoch 94, Loss: 1.166641354560852\n",
      "Epoch 95, Loss: 1.6104272603988647\n",
      "Epoch 96, Loss: 1.3122410774230957\n",
      "Epoch 97, Loss: 1.0585671663284302\n",
      "Epoch 98, Loss: 1.191056728363037\n",
      "Epoch 99, Loss: 0.9171413779258728\n",
      "Epoch 100, Loss: 1.281398892402649\n",
      "Validation Accuracy: 0.3598\n",
      "Epoch 1, Loss: 5.712221145629883\n",
      "Epoch 2, Loss: 3.8386573791503906\n",
      "Epoch 3, Loss: 3.781653881072998\n",
      "Epoch 4, Loss: 5.4524245262146\n",
      "Epoch 5, Loss: 4.35020112991333\n",
      "Epoch 6, Loss: 3.8680524826049805\n",
      "Epoch 7, Loss: 3.442821502685547\n",
      "Epoch 8, Loss: 4.1392364501953125\n",
      "Epoch 9, Loss: 3.8439953327178955\n",
      "Epoch 10, Loss: 3.002061128616333\n",
      "Epoch 11, Loss: 2.7457401752471924\n",
      "Epoch 12, Loss: 2.3386497497558594\n",
      "Epoch 13, Loss: 2.906773567199707\n",
      "Epoch 14, Loss: 2.5904440879821777\n",
      "Epoch 15, Loss: 2.969330072402954\n",
      "Epoch 16, Loss: 2.3607687950134277\n",
      "Epoch 17, Loss: 2.282477617263794\n",
      "Epoch 18, Loss: 2.251438617706299\n",
      "Epoch 19, Loss: 1.93805992603302\n",
      "Epoch 20, Loss: 2.022165060043335\n",
      "Epoch 21, Loss: 2.1716549396514893\n",
      "Epoch 22, Loss: 2.0571296215057373\n",
      "Epoch 23, Loss: 1.8715074062347412\n",
      "Epoch 24, Loss: 1.8651055097579956\n",
      "Epoch 25, Loss: 2.207805633544922\n",
      "Epoch 26, Loss: 1.7542047500610352\n",
      "Epoch 27, Loss: 1.7560043334960938\n",
      "Epoch 28, Loss: 1.7286732196807861\n",
      "Epoch 29, Loss: 1.655747413635254\n",
      "Epoch 30, Loss: 1.492256999015808\n",
      "Epoch 31, Loss: 1.957167148590088\n",
      "Epoch 32, Loss: 1.915635108947754\n",
      "Epoch 33, Loss: 1.7827489376068115\n",
      "Epoch 34, Loss: 1.6862839460372925\n",
      "Epoch 35, Loss: 2.0320935249328613\n",
      "Epoch 36, Loss: 1.704307198524475\n",
      "Epoch 37, Loss: 1.6234418153762817\n",
      "Epoch 38, Loss: 1.7200798988342285\n",
      "Epoch 39, Loss: 1.6404943466186523\n",
      "Epoch 40, Loss: 1.6274728775024414\n",
      "Epoch 41, Loss: 1.540250539779663\n",
      "Epoch 42, Loss: 1.6863020658493042\n",
      "Epoch 43, Loss: 1.7104618549346924\n",
      "Epoch 44, Loss: 1.9311491250991821\n",
      "Epoch 45, Loss: 1.5707510709762573\n",
      "Epoch 46, Loss: 1.782462239265442\n",
      "Epoch 47, Loss: 1.518654227256775\n",
      "Epoch 48, Loss: 1.4772629737854004\n",
      "Epoch 49, Loss: 1.57086181640625\n",
      "Epoch 50, Loss: 1.7394790649414062\n",
      "Epoch 51, Loss: 1.4402186870574951\n",
      "Epoch 52, Loss: 1.7374403476715088\n",
      "Epoch 53, Loss: 1.610836386680603\n",
      "Epoch 54, Loss: 1.689564824104309\n",
      "Epoch 55, Loss: 1.6468788385391235\n",
      "Epoch 56, Loss: 1.6250582933425903\n",
      "Epoch 57, Loss: 1.8116108179092407\n",
      "Epoch 58, Loss: 1.5218080282211304\n",
      "Epoch 59, Loss: 1.5026941299438477\n",
      "Epoch 60, Loss: 1.5098944902420044\n",
      "Epoch 61, Loss: 1.4420922994613647\n",
      "Epoch 62, Loss: 1.5672067403793335\n",
      "Epoch 63, Loss: 1.6240026950836182\n",
      "Epoch 64, Loss: 1.4336739778518677\n",
      "Epoch 65, Loss: 1.5017318725585938\n",
      "Epoch 66, Loss: 1.3020191192626953\n",
      "Epoch 67, Loss: 1.3582394123077393\n",
      "Epoch 68, Loss: 1.3947083950042725\n",
      "Epoch 69, Loss: 1.3728564977645874\n",
      "Epoch 70, Loss: 1.30532968044281\n",
      "Epoch 71, Loss: 1.438459873199463\n",
      "Epoch 72, Loss: 1.119025468826294\n",
      "Epoch 73, Loss: 1.464928150177002\n",
      "Epoch 74, Loss: 1.469184398651123\n",
      "Epoch 75, Loss: 1.4225945472717285\n",
      "Epoch 76, Loss: 1.2722474336624146\n",
      "Epoch 77, Loss: 1.1179550886154175\n",
      "Epoch 78, Loss: 1.5010545253753662\n",
      "Epoch 79, Loss: 1.2653509378433228\n",
      "Epoch 80, Loss: 1.2996934652328491\n",
      "Epoch 81, Loss: 1.3024665117263794\n",
      "Epoch 82, Loss: 1.3885077238082886\n",
      "Epoch 83, Loss: 1.1946443319320679\n",
      "Epoch 84, Loss: 1.2695937156677246\n",
      "Epoch 85, Loss: 0.9791914820671082\n",
      "Epoch 86, Loss: 1.1125158071517944\n",
      "Epoch 87, Loss: 1.3098385334014893\n",
      "Epoch 88, Loss: 1.069126009941101\n",
      "Epoch 89, Loss: 1.2504760026931763\n",
      "Epoch 90, Loss: 1.4253170490264893\n",
      "Epoch 91, Loss: 1.2852061986923218\n",
      "Epoch 92, Loss: 1.0605226755142212\n",
      "Epoch 93, Loss: 1.1724488735198975\n",
      "Epoch 94, Loss: 1.1146427392959595\n",
      "Epoch 95, Loss: 1.571861743927002\n",
      "Epoch 96, Loss: 1.1790062189102173\n",
      "Epoch 97, Loss: 1.227636456489563\n",
      "Epoch 98, Loss: 1.1306614875793457\n",
      "Epoch 99, Loss: 1.168142557144165\n",
      "Epoch 100, Loss: 1.28873610496521\n",
      "Validation Accuracy: 0.3439\n",
      "Epoch 1, Loss: 6.031498908996582\n",
      "Epoch 2, Loss: 6.0100884437561035\n",
      "Epoch 3, Loss: 4.33457088470459\n",
      "Epoch 4, Loss: 4.049261093139648\n",
      "Epoch 5, Loss: 4.2608137130737305\n",
      "Epoch 6, Loss: 4.971100807189941\n",
      "Epoch 7, Loss: 3.6154658794403076\n",
      "Epoch 8, Loss: 4.188538551330566\n",
      "Epoch 9, Loss: 3.4474785327911377\n",
      "Epoch 10, Loss: 2.722557544708252\n",
      "Epoch 11, Loss: 2.5952279567718506\n",
      "Epoch 12, Loss: 3.044795036315918\n",
      "Epoch 13, Loss: 2.951718807220459\n",
      "Epoch 14, Loss: 2.3976383209228516\n",
      "Epoch 15, Loss: 2.30647611618042\n",
      "Epoch 16, Loss: 2.4121689796447754\n",
      "Epoch 17, Loss: 2.319260835647583\n",
      "Epoch 18, Loss: 2.4940860271453857\n",
      "Epoch 19, Loss: 2.2538657188415527\n",
      "Epoch 20, Loss: 2.7715811729431152\n",
      "Epoch 21, Loss: 1.7078874111175537\n",
      "Epoch 22, Loss: 2.101914644241333\n",
      "Epoch 23, Loss: 1.951408863067627\n",
      "Epoch 24, Loss: 2.0536155700683594\n",
      "Epoch 25, Loss: 1.9520646333694458\n",
      "Epoch 26, Loss: 2.3457295894622803\n",
      "Epoch 27, Loss: 1.5240859985351562\n",
      "Epoch 28, Loss: 1.7190651893615723\n",
      "Epoch 29, Loss: 1.8945071697235107\n",
      "Epoch 30, Loss: 1.5202556848526\n",
      "Epoch 31, Loss: 1.6346704959869385\n",
      "Epoch 32, Loss: 1.5802321434020996\n",
      "Epoch 33, Loss: 1.8964753150939941\n",
      "Epoch 34, Loss: 1.543481707572937\n",
      "Epoch 35, Loss: 1.7728580236434937\n",
      "Epoch 36, Loss: 1.4695082902908325\n",
      "Epoch 37, Loss: 1.6317479610443115\n",
      "Epoch 38, Loss: 1.3181155920028687\n",
      "Epoch 39, Loss: 1.640606164932251\n",
      "Epoch 40, Loss: 1.9059594869613647\n",
      "Epoch 41, Loss: 1.4521933794021606\n",
      "Epoch 42, Loss: 1.6895294189453125\n",
      "Epoch 43, Loss: 1.5616461038589478\n",
      "Epoch 44, Loss: 1.6746718883514404\n",
      "Epoch 45, Loss: 1.6634485721588135\n",
      "Epoch 46, Loss: 1.4245485067367554\n",
      "Epoch 47, Loss: 1.4713562726974487\n",
      "Epoch 48, Loss: 1.30377197265625\n",
      "Epoch 49, Loss: 1.7435054779052734\n",
      "Epoch 50, Loss: 1.5410940647125244\n",
      "Epoch 51, Loss: 1.7557353973388672\n",
      "Epoch 52, Loss: 1.4729461669921875\n",
      "Epoch 53, Loss: 1.7058014869689941\n",
      "Epoch 54, Loss: 1.2987931966781616\n",
      "Epoch 55, Loss: 1.525467872619629\n",
      "Epoch 56, Loss: 1.8078986406326294\n",
      "Epoch 57, Loss: 1.542001485824585\n",
      "Epoch 58, Loss: 1.7134889364242554\n",
      "Epoch 59, Loss: 1.447257399559021\n",
      "Epoch 60, Loss: 1.3023332357406616\n",
      "Epoch 61, Loss: 1.5009382963180542\n",
      "Epoch 62, Loss: 1.6572099924087524\n",
      "Epoch 63, Loss: 1.4825187921524048\n",
      "Epoch 64, Loss: 1.3947457075119019\n",
      "Epoch 65, Loss: 1.3681367635726929\n",
      "Epoch 66, Loss: 1.3052852153778076\n",
      "Epoch 67, Loss: 1.4235053062438965\n",
      "Epoch 68, Loss: 1.3261719942092896\n",
      "Epoch 69, Loss: 1.42198646068573\n",
      "Epoch 70, Loss: 1.5990139245986938\n",
      "Epoch 71, Loss: 1.3249895572662354\n",
      "Epoch 72, Loss: 1.4198431968688965\n",
      "Epoch 73, Loss: 1.3044164180755615\n",
      "Epoch 74, Loss: 1.4097967147827148\n",
      "Epoch 75, Loss: 1.1760585308074951\n",
      "Epoch 76, Loss: 1.2353832721710205\n",
      "Epoch 77, Loss: 1.2792764902114868\n",
      "Epoch 78, Loss: 1.2667394876480103\n",
      "Epoch 79, Loss: 1.3691567182540894\n",
      "Epoch 80, Loss: 1.1119577884674072\n",
      "Epoch 81, Loss: 1.1275385618209839\n",
      "Epoch 82, Loss: 1.3047633171081543\n",
      "Epoch 83, Loss: 1.4550782442092896\n",
      "Epoch 84, Loss: 0.9598845839500427\n",
      "Epoch 85, Loss: 1.298506498336792\n",
      "Epoch 86, Loss: 1.1332429647445679\n",
      "Epoch 87, Loss: 1.156165599822998\n",
      "Epoch 88, Loss: 1.2632251977920532\n",
      "Epoch 89, Loss: 1.2835335731506348\n",
      "Epoch 90, Loss: 1.0570333003997803\n",
      "Epoch 91, Loss: 1.1668450832366943\n",
      "Epoch 92, Loss: 1.3746947050094604\n",
      "Epoch 93, Loss: 1.237149715423584\n",
      "Epoch 94, Loss: 1.1096159219741821\n",
      "Epoch 95, Loss: 1.0650458335876465\n",
      "Epoch 96, Loss: 1.0066455602645874\n",
      "Epoch 97, Loss: 1.128761649131775\n",
      "Epoch 98, Loss: 1.1679412126541138\n",
      "Epoch 99, Loss: 1.1557914018630981\n",
      "Epoch 100, Loss: 1.1768860816955566\n",
      "Validation Accuracy: 0.3386\n"
     ]
    }
   ],
   "source": [
    "best_val_accuracy = 0.0\n",
    "best_num_components = None\n",
    "\n",
    "for i in range(len(X_train_pca_vec)):\n",
    "    X_train_pca_i = X_train_pca_vec[i]\n",
    "    X_test_pca_i = X_test_pca_vec[i]\n",
    "    X_val_pca_i = X_val_pca_vec[i]\n",
    "\n",
    "    # Convert the PCA-transformed features and labels into PyTorch tensors\n",
    "    train_features = torch.tensor(X_train_pca_i, dtype=torch.float)\n",
    "    val_features = torch.tensor(X_val_pca_i, dtype=torch.float)\n",
    "    test_features = torch.tensor(X_test_pca_i, dtype=torch.float)\n",
    "    \n",
    "    train_labels = torch.tensor(y_train.values, dtype=torch.long)  # Assuming y_train is a pandas Series\n",
    "    val_labels = torch.tensor(y_val.values, dtype=torch.long)  # Assuming y_val is a pandas Series\n",
    "    test_labels = torch.tensor(y_test.values, dtype=torch.long)  # Assuming y_test is a pandas Series\n",
    "    \n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    val_dataset = TensorDataset(val_features, val_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "\n",
    "    inp_size = X_train_pca_i.shape[1]\n",
    "    best_params = {'input_size': inp_size, 'hidden_sizes': [256, 128], 'lr': 0.0001, 'epochs': 100, 'batch_size': 64, 'dropout_rate': 0.5}\n",
    "    val_accuracy, best_model = train_validate_model(train_dataset,val_dataset,device,best_params)\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_num_components = num_components[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c689ab01-e5d7-4e19-bc85-543568b51dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val accuracy:  0.3862433862433862\n",
      "Best number of components:  500\n"
     ]
    }
   ],
   "source": [
    "print(\"Best val accuracy: \",best_val_accuracy)\n",
    "print(\"Best number of components: \",best_num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b529db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "test_accuracy, test_predictions, test_actuals = test_model(best_model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
